{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "481UQoGsILtn",
    "outputId": "6e471217-5373-414d-ddee-7a8d7b10275c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "import os\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from sklearn.svm import SVC\n",
    "import json\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "jXgdiOZkIMms",
    "outputId": "bbfe2048-0a68-4f5e-9c0a-0296e29ed5e1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"database\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1450,\n        \"max\": 1459,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1458,\n          1451,\n          1455\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"no title\",\n          \"\\u0420\\u0443\\u0441\\u0430\\u043b \\u0411\\u0440\\u0430\\u0442\\u0441\\u043a \\u0411\\u041e-01 \\u2013 \\u0434\\u0430\\u0432\\u0430\\u0439\\u0442\\u0435 \\u043f\\u043e\\u043c\\u043d\\u0438\\u0442\\u044c \\u043e\\u0431 \\u043e\\u0444\\u0435\\u0440\\u0442\\u0435!\",\n          \"\\u0420\\u043e\\u0441\\u0441\\u0438\\u044f \\u0441\\u043d\\u0438\\u0437\\u0438\\u043b\\u0430 \\u0432\\u043b\\u043e\\u0436\\u0435\\u043d\\u0438\\u044f \\u0432 \\u0430\\u043c\\u0435\\u0440\\u0438\\u043a\\u0430\\u043d\\u0441\\u043a\\u0438\\u0439 \\u0433\\u043e\\u0441\\u0434\\u043e\\u043b\\u0433 \\u0432 \\u043c\\u0430\\u0435\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"body\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"\\u041e\\u043d\\u043b\\u0430\\u0439\\u043d-\\u0441\\u0435\\u0440\\u0432\\u0438\\u0441\\u044b \\u041c\\u0430\\u0433\\u043d\\u0438\\u0442\\u0430 \\u043f\\u0440\\u043e\\u0434\\u043e\\u043b\\u0436\\u0430\\u0442 \\u0440\\u0430\\u0437\\u0432\\u0438\\u0432\\u0430\\u0442\\u044c\\u0441\\u044f \\u0441 \\u0431\\u043e\\u043b\\u044c\\u0448\\u0438\\u043c \\u0444\\u043e\\u043a\\u0443\\u0441\\u043e\\u043c \\u043d\\u0430 \\u0440\\u0435\\u043d\\u0442\\u0430\\u0431\\u0435\\u043b\\u044c\\u043d\\u043e\\u0441\\u0442\\u044c - \\u0412\\u0435\\u043b\\u0435\\u0441 \\u041a\\u0430\\u043f\\u0438\\u0442\\u0430\\u043b\",\n          \"\\u041f\\u043e \\u043f\\u043e\\u0434\\u0441\\u0447\\u0451\\u0442\\u0430\\u043c \\u0430\\u043d\\u0430\\u043b\\u0438\\u0442\\u0438\\u043a\\u043e\\u0432 SberCIB \\u0447\\u0438\\u0441\\u0442\\u0430\\u044f \\u043f\\u0440\\u0438\\u0431\\u044b\\u043b\\u044c \\u041c\\u043e\\u0441\\u0431\\u0438\\u0440\\u0436\\u0438 \\u0437\\u0430 2\\u043a\\u043224 \\u0443\\u0432\\u0435\\u043b\\u0438\\u0447\\u0438\\u043b\\u0430\\u0441\\u044c \\u043d\\u0430 62%, \\u0430 \\u0440\\u0435\\u043d\\u0442\\u0430\\u0431\\u0435\\u043b\\u044c\\u043d\\u043e\\u0441\\u0442\\u044c \\u0430\\u043a\\u0446\\u0438\\u043e\\u043d\\u0435\\u0440\\u043d\\u043e\\u0433\\u043e \\u043a\\u0430\\u043f\\u0438\\u0442\\u0430\\u043b\\u0430 \\u2014 \\u0434\\u043e 34%\",\n          \"\\u0426\\u0411 \\u0441\\u043d\\u0438\\u0437\\u0438\\u043b \\u043a\\u043b\\u044e\\u0447\\u0435\\u0432\\u0443\\u044e \\u0441\\u0442\\u0430\\u0432\\u043a\\u0443 \\u0434\\u043e 9,5% \\u0433\\u043e\\u0434\\u043e\\u0432\\u044b\\u0445 \\u0421\\u043e\\u0432\\u0435\\u0442 \\u0434\\u0438\\u0440\\u0435\\u043a\\u0442\\u043e\\u0440\\u043e\\u0432 \\u0411\\u0430\\u043d\\u043a\\u0430 \\u0420\\u043e\\u0441\\u0441\\u0438\\u0438 \\u043f\\u0440\\u0438\\u043d\\u044f\\u043b \\u0440\\u0435\\u0448\\u0435\\u043d\\u0438\\u0435 \\u0441\\u043d\\u0438\\u0437\\u0438\\u0442\\u044c \\u043a\\u043b\\u044e\\u0447\\u0435\\u0432\\u0443\\u044e \\u0441\\u0442\\u0430\\u0432\\u043a\\u0443 \\u043d\\u0430 150 \\u0431. \\u043f., \\u0434\\u043e 9,5% \\u0433\\u043e\\u0434\\u043e\\u0432\\u044b\\u0445. \\u00ab\\u0412\\u043d\\u0435\\u0448\\u043d\\u0438\\u0435 \\u0443\\u0441\\u043b\\u043e\\u0432\\u0438\\u044f \\u0434\\u043b\\u044f \\u0440\\u043e\\u0441\\u0441\\u0438\\u0439\\u0441\\u043a\\u043e\\u0439 \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0438 \\u043e\\u0441\\u0442\\u0430\\u044e\\u0442\\u0441\\u044f \\u0441\\u043b\\u043e\\u0436\\u043d\\u044b\\u043c\\u0438 \\u0438 \\u0437\\u043d\\u0430\\u0447\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e \\u043e\\u0433\\u0440\\u0430\\u043d\\u0438\\u0447\\u0438\\u0432\\u0430\\u044e\\u0442 \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u0447\\u0435\\u0441\\u043a\\u0443\\u044e \\u0434\\u0435\\u044f\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e\\u0441\\u0442\\u044c. \\u0412\\u043c\\u0435\\u0441\\u0442\\u0435 \\u0441 \\u0442\\u0435\\u043c \\u0437\\u0430\\u043c\\u0435\\u0434\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0438\\u043d\\u0444\\u043b\\u044f\\u0446\\u0438\\u0438 \\u043f\\u0440\\u043e\\u0438\\u0441\\u0445\\u043e\\u0434\\u0438\\u0442 \\u0431\\u044b\\u0441\\u0442\\u0440\\u0435\\u0435, \\u0430 \\u0441\\u043d\\u0438\\u0436\\u0435\\u043d\\u0438\\u0435 \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u0447\\u0435\\u0441\\u043a\\u043e\\u0439 \\u0430\\u043a\\u0442\\u0438\\u0432\\u043d\\u043e\\u0441\\u0442\\u0438 \\u2014 \\u0432 \\u043c\\u0435\\u043d\\u044c\\u0448\\u0435\\u043c \\u043c\\u0430\\u0441\\u0448\\u0442\\u0430\\u0431\\u0435, \\u0447\\u0435\\u043c \\u0411\\u0430\\u043d\\u043a \\u0420\\u043e\\u0441\\u0441\\u0438\\u0438 \\u043e\\u0436\\u0438\\u0434\\u0430\\u043b \\u0432 \\u0430\\u043f\\u0440\\u0435\\u043b\\u0435. \\u041f\\u043e\\u0441\\u043b\\u0435\\u0434\\u043d\\u0438\\u0435 \\u0434\\u0430\\u043d\\u043d\\u044b\\u0435 \\u0443\\u043a\\u0430\\u0437\\u044b\\u0432\\u0430\\u044e\\u0442 \\u043d\\u0430 \\u043d\\u0438\\u0437\\u043a\\u0438\\u0435 \\u0442\\u0435\\u043a\\u0443\\u0449\\u0438\\u0435 \\u0442\\u0435\\u043c\\u043f\\u044b \\u043f\\u0440\\u0438\\u0440\\u043e\\u0441\\u0442\\u0430 \\u0446\\u0435\\u043d \\u0432 \\u043c\\u0430\\u0435 \\u0438 \\u043d\\u0430\\u0447\\u0430\\u043b\\u0435 \\u0438\\u044e\\u043d\\u044f. \\u042d\\u0442\\u043e\\u043c\\u0443 \\u0441\\u043f\\u043e\\u0441\\u043e\\u0431\\u0441\\u0442\\u0432\\u043e\\u0432\\u0430\\u043b\\u0438 \\u0434\\u0438\\u043d\\u0430\\u043c\\u0438\\u043a\\u0430 \\u043e\\u0431\\u043c\\u0435\\u043d\\u043d\\u043e\\u0433\\u043e \\u043a\\u0443\\u0440\\u0441\\u0430 \\u0440\\u0443\\u0431\\u043b\\u044f \\u0438 \\u0438\\u0441\\u0447\\u0435\\u0440\\u043f\\u0430\\u043d\\u0438\\u0435 \\u044d\\u0444\\u0444\\u0435\\u043a\\u0442\\u043e\\u0432 \\u0430\\u0436\\u0438\\u043e\\u0442\\u0430\\u0436\\u043d\\u043e\\u0433\\u043e \\u043f\\u043e\\u0442\\u0440\\u0435\\u0431\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u043a\\u043e\\u0433\\u043e \\u0441\\u043f\\u0440\\u043e\\u0441\\u0430 \\u0432 \\u0443\\u0441\\u043b\\u043e\\u0432\\u0438\\u044f\\u0445 \\u0437\\u0430\\u043c\\u0435\\u0442\\u043d\\u043e\\u0433\\u043e \\u0441\\u043d\\u0438\\u0436\\u0435\\u043d\\u0438\\u044f \\u0438\\u043d\\u0444\\u043b\\u044f\\u0446\\u0438\\u043e\\u043d\\u043d\\u044b\\u0445 \\u043e\\u0436\\u0438\\u0434\\u0430\\u043d\\u0438\\u0439 \\u043d\\u0430\\u0441\\u0435\\u043b\\u0435\\u043d\\u0438\\u044f \\u0438 \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441\\u0430\\u00bb, \\u2014 \\u043e\\u0442\\u043c\\u0435\\u0442\\u0438\\u043b\\u0438 \\u043f\\u0440\\u0435\\u0434\\u0441\\u0442\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u0438 \\u0440\\u0435\\u0433\\u0443\\u043b\\u044f\\u0442\\u043e\\u0440\\u0430. \\u041e \\u0441\\u043d\\u0438\\u0436\\u0435\\u043d\\u0438\\u0438 \\u0441\\u0442\\u0430\\u0432\\u043a\\u0438 \\u043f\\u0438\\u0441\\u0430\\u043b\\u0438 \\u0438 \\u0430\\u043d\\u0430\\u043b\\u0438\\u0442\\u0438\\u043a\\u0438 \\u0422\\u0438\\u043d\\u044c\\u043a\\u043e\\u0444\\u0444 \\u0418\\u043d\\u0432\\u0435\\u0441\\u0442\\u0438\\u0446\\u0438\\u0439. \\u00ab\\u041f\\u043e\\u0441\\u043b\\u0435\\u0434\\u043d\\u0438\\u0435 \\u043d\\u0435\\u0434\\u0435\\u043b\\u044c\\u043d\\u044b\\u0435 \\u0434\\u0430\\u043d\\u043d\\u044b\\u0435 \\u043f\\u043e \\u0438\\u043d\\u0444\\u043b\\u044f\\u0446\\u0438\\u0438 \\u0443\\u043a\\u0430\\u0437\\u044b\\u0432\\u0430\\u044e\\u0442 \\u043d\\u0430 \\u0437\\u043d\\u0430\\u0447\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e\\u0435 \\u0437\\u0430\\u043c\\u0435\\u0434\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0442\\u0435\\u043a\\u0443\\u0449\\u0438\\u0445 \\u0442\\u0435\\u043c\\u043f\\u043e\\u0432 \\u0440\\u043e\\u0441\\u0442\\u0430 \\u0446\\u0435\\u043d. \\u041e\\u0441\\u043b\\u0430\\u0431\\u043b\\u0435\\u043d\\u0438\\u044e \\u0438\\u043d\\u0444\\u043b\\u044f\\u0446\\u0438\\u043e\\u043d\\u043d\\u043e\\u0433\\u043e \\u0434\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u044f \\u0441\\u043f\\u043e\\u0441\\u043e\\u0431\\u0441\\u0442\\u0432\\u0443\\u044e\\u0442 \\u0434\\u0438\\u043d\\u0430\\u043c\\u0438\\u043a\\u0430 \\u043e\\u0431\\u043c\\u0435\\u043d\\u043d\\u043e\\u0433\\u043e \\u043a\\u0443\\u0440\\u0441\\u0430 \\u0440\\u0443\\u0431\\u043b\\u044f \\u043d\\u0430\\u0440\\u044f\\u0434\\u0443 \\u0441 \\u0437\\u0430\\u043c\\u0435\\u0442\\u043d\\u044b\\u043c \\u0441\\u043d\\u0438\\u0436\\u0435\\u043d\\u0438\\u0435\\u043c \\u0438\\u043d\\u0444\\u043b\\u044f\\u0446\\u0438\\u043e\\u043d\\u043d\\u044b\\u0445 \\u043e\\u0436\\u0438\\u0434\\u0430\\u043d\\u0438\\u0439 \\u043d\\u0430\\u0441\\u0435\\u043b\\u0435\\u043d\\u0438\\u044f \\u0438 \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441\\u0430\\u00bb, \\u2014 \\u043e\\u0442\\u043c\\u0435\\u0447\\u0430\\u043b\\u0438 \\u0430\\u043d\\u0430\\u043b\\u0438\\u0442\\u0438\\u043a\\u0438. \\u041f\\u043e \\u0438\\u0445 \\u043c\\u043d\\u0435\\u043d\\u0438\\u044e, \\u043f\\u043e \\u0438\\u0442\\u043e\\u0433\\u0430\\u043c \\u0433\\u043e\\u0434\\u0430 \\u043a\\u043b\\u044e\\u0447\\u0435\\u0432\\u0430\\u044f \\u0441\\u0442\\u0430\\u0432\\u043a\\u0430 \\u0441\\u043e\\u0441\\u0442\\u0430\\u0432\\u0438\\u0442 8\\u20149%. \\u041d\\u0430\\u043f\\u043e\\u043c\\u043d\\u0438\\u043c, 11 \\u0444\\u0435\\u0432\\u0440\\u0430\\u043b\\u044f \\u043a\\u043b\\u044e\\u0447\\u0435\\u0432\\u0430\\u044f \\u0441\\u0442\\u0430\\u0432\\u043a\\u0430 \\u0431\\u044b\\u043b\\u0430 \\u0443\\u0441\\u0442\\u0430\\u043d\\u043e\\u0432\\u043b\\u0435\\u043d\\u0430 \\u043d\\u0430 \\u0443\\u0440\\u043e\\u0432\\u043d\\u0435 9,5%. \\u0422\\u043e\\u0433\\u0434\\u0430 \\u044d\\u0442\\u043e \\u0441\\u0442\\u0430\\u043b\\u043e \\u043c\\u0430\\u043a\\u0441\\u0438\\u043c\\u0443\\u043c\\u043e\\u043c \\u0441 \\u0432\\u0435\\u0441\\u043d\\u044b 2017 \\u0433\\u043e\\u0434\\u0430. 28 \\u0444\\u0435\\u0432\\u0440\\u0430\\u043b\\u044f \\u0426\\u0411 \\u043f\\u043e\\u0432\\u044b\\u0441\\u0438\\u043b \\u0441\\u0442\\u0430\\u0432\\u043a\\u0443 \\u0441\\u0440\\u0430\\u0437\\u0443 \\u0434\\u043e 20%, \\u043e\\u0431\\u044a\\u044f\\u0441\\u043d\\u0438\\u0432 \\u0440\\u0435\\u0448\\u0435\\u043d\\u0438\\u0435 \\u043a\\u0430\\u0440\\u0434\\u0438\\u043d\\u0430\\u043b\\u044c\\u043d\\u044b\\u043c \\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0435\\u043c \\u0432\\u043d\\u0435\\u0448\\u043d\\u0438\\u0445 \\u0443\\u0441\\u043b\\u043e\\u0432\\u0438\\u0439. 8 \\u0430\\u043f\\u0440\\u0435\\u043b\\u044f \\u0440\\u0435\\u0433\\u0443\\u043b\\u044f\\u0442\\u043e\\u0440 \\u0441\\u043d\\u0438\\u0437\\u0438\\u043b \\u0441\\u0442\\u0430\\u0432\\u043a\\u0443 \\u0434\\u043e 17%, 29 \\u0430\\u043f\\u0440\\u0435\\u043b\\u044f \\u2014 \\u0434\\u043e 14%, \\u0430 26 \\u043c\\u0430\\u044f \\u043d\\u0430 \\u0432\\u043d\\u0435\\u043e\\u0447\\u0435\\u0440\\u0435\\u0434\\u043d\\u043e\\u043c \\u0437\\u0430\\u0441\\u0435\\u0434\\u0430\\u043d\\u0438\\u0438 \\u2014 \\u0434\\u043e 11% \\u0433\\u043e\\u0434\\u043e\\u0432\\u044b\\u0445.  #\\u043d\\u043e\\u0432\\u043e\\u0441\\u0442\\u0438\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"2024-08-23\",\n          \"2022-06-10\",\n          \"2024-10-17\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tags\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"no tags\",\n          \"['\\u043c\\u0430\\u043a\\u0440\\u043e', '\\u0435\\u0432\\u0440\\u043e\\u043f\\u0430']\",\n          \"['\\u043e\\u0442\\u0447\\u0435\\u0442']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"smart_lab\",\n          \"rdv\",\n          \"bcs\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": -1,\n        \"max\": 1,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1,\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-1a83496c-1d64-4048-bc92-908117de5410\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>date</th>\n",
       "      <th>tags</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>1450</td>\n",
       "      <td>no title</td>\n",
       "      <td>​​Не стальные результаты ММК Как высокая ставк...</td>\n",
       "      <td>2024-10-17</td>\n",
       "      <td>['отчет']</td>\n",
       "      <td>t_invest</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>1451</td>\n",
       "      <td>no title</td>\n",
       "      <td>По подсчётам аналитиков SberCIB чистая прибыль...</td>\n",
       "      <td>2024-08-23</td>\n",
       "      <td>no tags</td>\n",
       "      <td>smart_lab</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>1452</td>\n",
       "      <td>Русал Братск БО-01 – давайте помнить об оферте!</td>\n",
       "      <td>Русал Братск снизит величину купона до 0,1% го...</td>\n",
       "      <td>2019-04-05 00:00:00</td>\n",
       "      <td>['РусалБр Б-1 об RU000A0JWDN6']</td>\n",
       "      <td>bcs</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>1453</td>\n",
       "      <td>Россия снизила вложения в американский госдолг...</td>\n",
       "      <td>Американское Казначейство обнародовало месячны...</td>\n",
       "      <td>2020-07-17 00:00:00</td>\n",
       "      <td>['USD/RUB USD000UTSTOM', 'USD/RUB TOD USD00000...</td>\n",
       "      <td>bcs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>1454</td>\n",
       "      <td>no title</td>\n",
       "      <td>Высокая ключевая ставка усилит давление на про...</td>\n",
       "      <td>2024-05-28</td>\n",
       "      <td>no tags</td>\n",
       "      <td>smart_lab</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>1455</td>\n",
       "      <td>no title</td>\n",
       "      <td>ЦБ снизил ключевую ставку до 9,5% годовых Сове...</td>\n",
       "      <td>2022-06-10</td>\n",
       "      <td>['новости']</td>\n",
       "      <td>t_invest</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>1456</td>\n",
       "      <td>no title</td>\n",
       "      <td>​​Внеплановое заседание ЕЦБ Регулятор решил об...</td>\n",
       "      <td>2022-06-15</td>\n",
       "      <td>['макро', 'европа']</td>\n",
       "      <td>t_analytic</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1457</td>\n",
       "      <td>no title</td>\n",
       "      <td>Крупнейшее с 1994 года повышение ставки ФРС С...</td>\n",
       "      <td>2022-06-15</td>\n",
       "      <td>[]</td>\n",
       "      <td>rdv</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>1458</td>\n",
       "      <td>no title</td>\n",
       "      <td>Онлайн-сервисы Магнита продолжат развиваться с...</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>no tags</td>\n",
       "      <td>smart_lab</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1459</td>\n",
       "      <td>no title</td>\n",
       "      <td>​​Что происходит на американском рынке? Вчера ...</td>\n",
       "      <td>2022-06-15</td>\n",
       "      <td>['рынки', 'сша']</td>\n",
       "      <td>t_invest</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a83496c-1d64-4048-bc92-908117de5410')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-1a83496c-1d64-4048-bc92-908117de5410 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-1a83496c-1d64-4048-bc92-908117de5410');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-01cf64a8-a154-4c84-b346-292d5668205c\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-01cf64a8-a154-4c84-b346-292d5668205c')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-01cf64a8-a154-4c84-b346-292d5668205c button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "      Unnamed: 0                                              title  \\\n",
       "1449        1450                                           no title   \n",
       "1450        1451                                           no title   \n",
       "1451        1452    Русал Братск БО-01 – давайте помнить об оферте!   \n",
       "1452        1453  Россия снизила вложения в американский госдолг...   \n",
       "1453        1454                                           no title   \n",
       "1454        1455                                           no title   \n",
       "1455        1456                                           no title   \n",
       "1456        1457                                           no title   \n",
       "1457        1458                                           no title   \n",
       "1458        1459                                           no title   \n",
       "\n",
       "                                                   body                 date  \\\n",
       "1449  ​​Не стальные результаты ММК Как высокая ставк...           2024-10-17   \n",
       "1450  По подсчётам аналитиков SberCIB чистая прибыль...           2024-08-23   \n",
       "1451  Русал Братск снизит величину купона до 0,1% го...  2019-04-05 00:00:00   \n",
       "1452  Американское Казначейство обнародовало месячны...  2020-07-17 00:00:00   \n",
       "1453  Высокая ключевая ставка усилит давление на про...           2024-05-28   \n",
       "1454  ЦБ снизил ключевую ставку до 9,5% годовых Сове...           2022-06-10   \n",
       "1455  ​​Внеплановое заседание ЕЦБ Регулятор решил об...           2022-06-15   \n",
       "1456   Крупнейшее с 1994 года повышение ставки ФРС С...           2022-06-15   \n",
       "1457  Онлайн-сервисы Магнита продолжат развиваться с...           2022-05-11   \n",
       "1458  ​​Что происходит на американском рынке? Вчера ...           2022-06-15   \n",
       "\n",
       "                                                   tags      source  target  \n",
       "1449                                          ['отчет']    t_invest      -1  \n",
       "1450                                            no tags   smart_lab       1  \n",
       "1451                    ['РусалБр Б-1 об RU000A0JWDN6']         bcs      -1  \n",
       "1452  ['USD/RUB USD000UTSTOM', 'USD/RUB TOD USD00000...         bcs       0  \n",
       "1453                                            no tags   smart_lab       1  \n",
       "1454                                        ['новости']    t_invest       1  \n",
       "1455                                ['макро', 'европа']  t_analytic      -1  \n",
       "1456                                                 []         rdv      -1  \n",
       "1457                                            no tags   smart_lab       1  \n",
       "1458                                   ['рынки', 'сша']    t_invest      -1  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database = pd.read_excel('train_1459.xlsx')\n",
    "database.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQRUn_7OZmoI"
   },
   "source": [
    "In the dataset the disclamers of the souces entities , and not needed quater marks are noticed . So they are deleted. Words and punctuation were saved. We also preserve the case of the source text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "TaYKl2DBKsK-"
   },
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "    # function for text clearning\n",
    "    s=s.replace('no title','')\n",
    "    s=s.replace('no tags','')\n",
    "    s=s.replace('[]','')\n",
    "    s=s.split('$$$ БКС Мир инвестиций $$$ Инвестировать легче вместе')[0]\n",
    "    s=s.replace('$$$','.')\n",
    "    s=re.sub('\\s*\"','',s)\n",
    "    s=s.replace('#','')\n",
    "    s=s.replace(' • ',', ')\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "OASDFN2zlBqL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(database, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "D210ZoGKJQ4m",
    "outputId": "aeb5ee2c-a8be-4a72-c302-98972db5feef"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Самолет (SMLT) в 4кв2023 собирается побить рекорд 3кв. Менеджмент подтверждает цель в 1 600 тыс. кв. м продаж за весь 2023 год. аналитика факты Видимо, менеджмент видит уже по октябрьским цифрам, что даже после роста ключевой ставки продажи на первичке идут хорошо. Первичный рынок, особенно массовый сегмент, на котором работает Самолет — бенефициар программ льготной ипотеки. Цель на год 1600 кв.м.:, по итогам 9 месяцев есть 973 тыс. кв. м., консолидация МИЦ даст примерно 100 тыс. кв. м, таким образом, продажи Самолёта без МИЦ в 4 квартале могут составить порядка 527 тыс. кв. м - на 25% больше, чем в рекордном 3кв2023. ['аналитика', 'факты']\""
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text_extraction'] = train.title + ' ' + train.body + ' ' + train.tags\n",
    "train['text_extraction_cleaned']= train.text_extraction.map(preprocess)\n",
    "train['text_extraction_cleaned'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "fhmLwiWGJ-fx",
    "outputId": "aa864975-cae7-4060-b8f9-f41621f7788a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Самолет (SMLT) в 4кв2023 собирается побить рекорд 3кв. Менеджмент подтверждает цель в 1 600 тыс. кв. м продаж за весь 2023 год. аналитика факты Видимо, менеджмент видит уже по октябрьским цифрам, что даже после роста ключевой ставки продажи на первичке идут хорошо. Первичный рынок, особенно массовый сегмент, на котором работает Самолет — бенефициар программ льготной ипотеки. Цель на год 1600 кв.м.:, по итогам 9 месяцев есть 973 тыс. кв. м., консолидация МИЦ даст примерно 100 тыс. кв. м, таким образом, продажи Самолёта без МИЦ в 4 квартале могут составить порядка 527 тыс. кв. м - на 25% больше, чем в рекордном 3кв2023. ['аналитика', 'факты']\""
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text_extraction_cleaned'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8KmpcNdbfH5"
   },
   "source": [
    "## Dataset analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esoke6T7dHiE"
   },
   "source": [
    "Basic statistics :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "HcCQ9BNRh80_"
   },
   "outputs": [],
   "source": [
    "def words(i):\n",
    "    # a function for splitting text into words\n",
    "    regex = re.compile(r'[А-Яа-яA-zёЁ-]+')\n",
    "    i= \" \".join(regex.findall(i))\n",
    "    tokens=word_tokenize(i.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "Zq5GNbgki-uh"
   },
   "outputs": [],
   "source": [
    "train['tokens'] = train.text_extraction_cleaned.map(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHKiVOk7dGtH",
    "outputId": "ff4294ad-b573-495f-953b-efa2b0862440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the number of missing values : \n",
      "Number of missing values  in text_extraction: 0\n",
      "Number of missing values in assessment of tonality : 0\n",
      "\n",
      "Number of examples in dataset: 1167\n",
      "\n",
      "Mean length of text_extraction: 83.0\n",
      "\n",
      "Number of unique words : 13573\n"
     ]
    }
   ],
   "source": [
    "example_number = train.shape[0]\n",
    "\n",
    "\n",
    "mean_length_text_extract=[]\n",
    "for i in train.tokens:\n",
    "    mean_length_text_extract.append(len(i))\n",
    "mean_length_text_extract=np.mean(mean_length_text_extract)\n",
    "\n",
    "number_unique_words=set()\n",
    "for i in train.tokens:\n",
    "    for j in i:\n",
    "        number_unique_words.add(j)\n",
    "number_unique_words=len(number_unique_words)\n",
    "\n",
    "print('Checking the number of missing values : ' )\n",
    "print(f'Number of missing values  in text_extraction: {train.text_extraction_cleaned.isnull().sum()}',f'Number of missing values in assessment of tonality : {train.target.isnull().sum()}','',sep='\\n')\n",
    "print(f'Number of examples in dataset: {example_number}','',sep='\\n' )\n",
    "\n",
    "print(f'Mean length of text_extraction: {round(mean_length_text_extract,0)}','',sep='\\n')\n",
    "\n",
    "print(f'Number of unique words : {number_unique_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuNDxXN3mGu9"
   },
   "source": [
    "Target column analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "7bJXCvVGmX4N"
   },
   "outputs": [],
   "source": [
    "def year_extraction(s):\n",
    "    # function for year extraction from the date column\n",
    "    if type(s) == int:\n",
    "       s=str(s)\n",
    "    else:\n",
    "       s=s.split('-')[0]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RW7Q2km9mfv3",
    "outputId": "a60e4cb3-34fc-4df3-a471-8044777757cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2022', '2023', '2021', '2024', '2020', '2012', '2013', '2019',\n",
       "       '2011'], dtype=object)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['year']= train.date.map(year_extraction)\n",
    "train['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_aBW_LAl1Ey",
    "outputId": "a13c5a79-c105-4680-c259-a92e892cd580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_distribution: target\n",
      " 0    451\n",
      " 1    426\n",
      "-1    290\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_distribution =  train.target.value_counts()\n",
    "print(f'Class_distribution: {class_distribution}','',sep='\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "IVHqdumpA_Lz",
    "outputId": "01fb3615-143f-47ac-8740-0a241fde2b63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distirbution of the target variable')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "train.target.plot.hist()\n",
    "plt.ylabel('Number of examples')\n",
    "plt.xlabel('Polarity')\n",
    "plt.xticks(np.arange(-1,2,1))\n",
    "plt.title('Distirbution of the target variable')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "8xns_-8enH1u",
    "outputId": "28155960-a0b5-4650-93cd-5475319f5a24"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"train\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": [\n        \"year\",\n        \"\"\n      ],\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"2023\",\n          \"2012\",\n          \"2021\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"target\",\n        -1\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 49.97866211361271,\n        \"min\": 1.0,\n        \"max\": 118.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1.0,\n          8.0,\n          118.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"target\",\n        0\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 67.56190576879186,\n        \"min\": 1.0,\n        \"max\": 150.0,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          145.0,\n          3.0,\n          9.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"target\",\n        1\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 67.3196002025307,\n        \"min\": 1.0,\n        \"max\": 158.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          2.0,\n          112.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-a598392e-3632-4054-886a-c99099cd9360\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th>-1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>60.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>94.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>118.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a598392e-3632-4054-886a-c99099cd9360')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-a598392e-3632-4054-886a-c99099cd9360 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-a598392e-3632-4054-886a-c99099cd9360');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-be6f790d-5b36-4a0e-a5d6-c1b6a0717d7c\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-be6f790d-5b36-4a0e-a5d6-c1b6a0717d7c')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-be6f790d-5b36-4a0e-a5d6-c1b6a0717d7c button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "       target              \n",
       "target     -1      0      1\n",
       "year                       \n",
       "2011      NaN    1.0    1.0\n",
       "2012      NaN    3.0    2.0\n",
       "2013      NaN    5.0    NaN\n",
       "2019      1.0    6.0    3.0\n",
       "2020      8.0    8.0   14.0\n",
       "2021      9.0    9.0    7.0\n",
       "2022     60.0  124.0  112.0\n",
       "2023     94.0  145.0  129.0\n",
       "2024    118.0  150.0  158.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.pivot_table(index='year', columns ='target',aggfunc={'target':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "Eyhen5d3E8Fu",
    "outputId": "62ff5016-1bd4-4c02-e67b-340d6dd880d5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "year\n",
       "2011      2\n",
       "2012      5\n",
       "2013      5\n",
       "2019     10\n",
       "2020     30\n",
       "2021     25\n",
       "2022    296\n",
       "2023    368\n",
       "2024    426\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.year.value_counts().sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38tIs6i1uQJh"
   },
   "source": [
    " ## 1. Feature-based approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "bVsJuwQYMnd7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-multilingual-cased', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "q4Il927zrdRw"
   },
   "outputs": [],
   "source": [
    "def get_sentence_embedding_bert(text, bert_model):\n",
    "    # getting sentence embeddings from BERT model\n",
    "    tokens = tokenizer(text,add_special_tokens = True, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**tokens)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    return last_hidden_state[:, 0, :].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "eRRWe_9GrlOL",
    "outputId": "91a4799f-dfa3-43c1-86f1-592092ccf866"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"«Сбер» по заказу «Абрау-Дюрсо» создал ИИ-модель по прогнозированию спроса Алгоритм предсказывает спрос на продукцию компании с высокой точностью благодаря глубокому анализу ретроспективных данных и дополнительных параметров\\n ['АбрауДюрсо', 'Сбербанк']\""
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing train dataset to embedding extraction\n",
    "sentences_train = train.text_extraction_cleaned.values\n",
    "labels_train = train.target.values\n",
    "sentences_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "P8QGfEFEso4a"
   },
   "outputs": [],
   "source": [
    "# preparing test dataset to embedding extraction\n",
    "test['text_extraction'] = test.title + ' ' + test.body + ' ' + test.tags\n",
    "test['text_extraction_cleaned']= test.text_extraction.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "fomXSqHosjq1",
    "outputId": "e382b73d-94fe-4bcc-8e5a-0a455f89f7dc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' ЦБ РФ далек от ситуации, когда ключевая ставка теряет свою эффективность — Набиуллина '"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_test = test.text_extraction_cleaned.values\n",
    "labels_test = test.target.values\n",
    "sentences_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "-R0UAPg2tju8"
   },
   "outputs": [],
   "source": [
    "sent_embeddings_train = []\n",
    "for text in sentences_train:\n",
    "    sentence_embedding = get_sentence_embedding_bert(text, bert_model)\n",
    "    sent_embeddings_train.append(sentence_embedding)\n",
    "\n",
    "\n",
    "sent_embeddings_test = []\n",
    "for text in sentences_test:\n",
    "    sentence_embedding = get_sentence_embedding_bert(text, bert_model)\n",
    "    sent_embeddings_test.append(sentence_embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "bApWt7xREpqd"
   },
   "outputs": [],
   "source": [
    "sent_embeddings_train_list=[]\n",
    "for i in range(len(sent_embeddings_train)):\n",
    "     sent_embeddings_train_list.append(sent_embeddings_train[i][0])\n",
    "\n",
    "\n",
    "sent_embeddings_test_list=[]\n",
    "for i in range(len(sent_embeddings_test)):\n",
    "     sent_embeddings_test_list.append(sent_embeddings_test[i][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2y8NlrU1ODX"
   },
   "source": [
    "t-SNE (t-Distributed Stochastic Neighbor Embedding) is a widely used dimensionality reduction technique for visualizing high-dimensional data. While t-SNE is primarily used for visualization, it can also be used to generate embeddings. The process involves applying t-SNE to reduce the dimensionality of the data and obtaining a lower-dimensional embedding that captures the inherent structure of the original high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "80ehxLn51s31",
    "outputId": "5dd5baf7-bc05-49b3-8685-290130cfbd6b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"abc79a3b-f585-442d-a2fd-39de3efb13f9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"abc79a3b-f585-442d-a2fd-39de3efb13f9\")) {                    Plotly.newPlot(                        \"abc79a3b-f585-442d-a2fd-39de3efb13f9\",                        [{\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003elabel=%{marker.color}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[0,0,0,-1,-1,0,-1,0,1,-1,1,1,1,-1,0,1,1,-1,0,1,-1,1,-1,0,0,-1,0,0,0,0,0,-1,1,1,0,1,1,1,1,-1,-1,-1,0,-1,0,0,1,-1,0,1,0,-1,-1,1,0,0,1,0,0,-1,1,0,-1,1,0,0,-1,0,0,0,-1,1,0,1,0,-1,1,0,1,0,0,0,0,0,0,1,1,0,1,0,0,0,1,1,1,1,1,1,1,0,-1,1,0,0,-1,-1,1,0,1,0,-1,0,0,1,0,1,1,1,-1,-1,-1,-1,0,1,-1,1,0,-1,0,0,0,1,0,1,0,1,-1,-1,1,-1,0,-1,0,0,1,0,-1,-1,0,0,1,0,1,-1,0,1,0,1,1,1,1,1,1,1,1,-1,0,-1,0,0,1,0,1,1,0,0,0,0,0,0,1,1,1,-1,0,-1,0,1,1,1,0,1,0,0,-1,-1,1,1,1,0,0,-1,-1,0,0,0,-1,0,1,0,1,0,-1,1,0,-1,0,0,1,0,-1,0,-1,-1,0,0,1,-1,1,1,-1,1,-1,0,0,1,1,1,-1,-1,1,1,0,-1,0,-1,1,1,0,0,0,-1,0,1,0,0,-1,1,-1,0,1,-1,0,1,1,1,0,1,1,-1,0,0,1,1,1,1,-1,1,1,1,0,1,1,1,0,1,1,0,0,0,0,0,-1,-1,-1,0,1,-1,0,1,1,-1,0,0,1,0,0,0,-1,1,1,0,1,0,0,-1,1,0,0,-1,-1,0,0,1,-1,1,0,-1,0,-1,1,-1,0,0,1,1,-1,1,-1,1,1,-1,1,1,1,-1,1,-1,1,-1,-1,1,1,-1,1,0,0,1,-1,1,0,0,1,1,0,-1,1,0,1,1,-1,-1,1,0,1,0,-1,1,1,0,-1,1,0,0,1,0,1,1,1,-1,1,-1,1,-1,0,0,0,0,-1,1,-1,0,0,0,0,1,-1,-1,-1,-1,1,1,1,-1,0,-1,1,0,0,0,0,-1,-1,0,1,1,0,1,1,1,1,0,0,-1,0,0,0,1,-1,0,0,-1,1,-1,-1,0,0,0,0,0,-1,0,0,0,0,-1,0,0,-1,0,1,1,1,1,-1,0,-1,1,0,0,1,1,1,-1,0,0,1,-1,-1,0,0,1,1,1,0,0,1,1,-1,1,0,1,-1,0,1,1,0,0,-1,1,0,-1,0,0,1,-1,1,1,1,1,-1,0,1,0,-1,0,0,1,1,1,0,0,-1,0,1,1,0,0,0,-1,-1,-1,1,0,0,1,0,1,0,1,0,1,0,0,0,-1,1,1,0,0,-1,0,1,0,-1,-1,0,0,-1,-1,0,1,0,0,0,1,-1,0,0,-1,0,0,0,-1,-1,1,0,0,-1,-1,1,0,1,-1,1,1,1,1,0,0,-1,1,-1,1,0,-1,0,-1,0,0,0,1,0,1,0,1,-1,-1,-1,-1,-1,0,1,1,0,1,1,-1,1,-1,0,0,0,1,0,0,1,0,0,1,1,0,1,1,-1,0,0,-1,0,-1,0,0,1,0,0,0,-1,-1,1,0,1,0,1,1,1,-1,1,-1,1,-1,-1,0,1,-1,0,0,0,1,0,1,1,0,-1,1,-1,-1,0,-1,1,1,-1,-1,-1,1,1,0,-1,1,0,0,-1,0,0,0,1,0,1,0,1,0,1,1,-1,-1,0,1,-1,0,0,1,0,0,1,1,1,-1,-1,1,0,0,0,0,1,0,0,0,0,-1,0,0,1,1,1,1,-1,-1,0,0,1,0,-1,1,-1,1,0,1,1,1,0,0,0,1,0,1,1,0,-1,1,1,-1,1,1,1,0,-1,1,0,1,-1,1,-1,1,1,1,1,-1,1,0,0,1,0,1,0,1,1,1,1,0,1,0,0,0,0,-1,1,0,-1,0,1,-1,0,1,-1,1,1,1,1,0,0,-1,1,0,0,1,1,1,-1,0,0,1,1,1,1,0,-1,0,1,-1,0,0,1,1,-1,0,0,1,0,1,-1,-1,0,1,0,1,-1,1,1,0,1,1,1,1,-1,-1,-1,-1,1,1,0,-1,1,-1,1,1,1,0,0,0,0,1,1,0,1,0,1,1,0,-1,0,-1,1,-1,1,-1,-1,-1,1,1,-1,1,0,1,1,1,-1,1,0,1,1,0,0,-1,-1,0,0,0,0,0,1,1,0,0,0,0,0,1,-1,0,1,1,1,-1,0,1,-1,-1,-1,0,-1,0,0,0,-1,1,-1,0,0,1,0,0,1,-1,-1,-1,-1,1,0,1,1,1,0,-1,0,0,1,-1,1,-1,0,-1,0,0,1,-1,-1,0,0,0,1,1,0,1,1,0,0,1,-1,-1,1,1,0,0,1,0,0,0,-1,0,0,0,0,1,1,1,0,1,0,-1,0,-1,0,0,0,-1,1,-1,-1,1,1,0,1,1,1,0,0,-1,-1,0,1,0,-1,-1,-1,1,0,1,1,0,-1,0,-1,-1,1,0,0,-1,0,-1,1,1,0,-1,-1,0,1,1,-1,-1,-1,-1,0,1,0,-1,0,1,-1,1,0,1,1,1,-1,-1,0,-1,1,-1,0,1,0,-1,0,0,1,1,0,1,1,0,0,0,0,1,-1,1,-1,-1,0,-1,1,1,0,0,1,1,0,1,1,1,0,-1,0,0,1,-1,-1,1,1,0,1,-1,1,0,0,0,-1,0,0,0,1,1,0,0,0,1,-1,1,-1,0,0,0,0,-1,-1,0,0,-1,1,-1,1,-1,0,1,0,1,1,-1,0,1,-1,-1,-1,1,-1,1,1,-1,-1,0,0,1,1,1,1,-1,0],\"coloraxis\":\"coloraxis\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"showlegend\":false,\"x\":[1.5649879e-7,1.5718041e-7,1.5652594e-7,1.5650853e-7,1.5646981e-7,1.565068e-7,1.5650335e-7,1.5650578e-7,1.565021e-7,1.5651396e-7,1.5701336e-7,1.5650213e-7,1.565345e-7,1.5649499e-7,1.5650559e-7,1.5652425e-7,1.5551831e-7,1.5649607e-7,1.5651165e-7,1.5652499e-7,1.5650326e-7,1.5647039e-7,1.5650798e-7,1.5650652e-7,1.5652209e-7,1.5671772e-7,1.5650484e-7,1.5650072e-7,1.565215e-7,1.5651771e-7,1.5650194e-7,1.5652168e-7,1.5651628e-7,1.5650336e-7,1.5387589e-7,1.5649756e-7,1.5649954e-7,1.5650487e-7,1.5650507e-7,1.5551602e-7,1.5646938e-7,1.5649634e-7,1.5650274e-7,1.5650298e-7,1.5456489e-7,1.5650132e-7,1.565044e-7,1.5650144e-7,1.5651041e-7,1.5686325e-7,1.5650178e-7,1.5652245e-7,1.5650805e-7,1.5388702e-7,1.5649874e-7,1.5651015e-7,1.568464e-7,1.5651138e-7,1.5654095e-7,1.5650173e-7,1.5566216e-7,1.5651443e-7,1.5650717e-7,1.5650629e-7,1.5650497e-7,1.5650559e-7,1.5651793e-7,1.565161e-7,1.5650198e-7,1.5650053e-7,1.5650838e-7,1.5650478e-7,1.5650576e-7,1.5672742e-7,1.5668168e-7,1.5651275e-7,1.5650316e-7,1.565052e-7,1.5650465e-7,1.565019e-7,1.5651317e-7,1.5653886e-7,1.565033e-7,1.5650475e-7,1.5650681e-7,1.5724028e-7,1.5650397e-7,1.5650458e-7,1.564957e-7,1.5650488e-7,1.565204e-7,1.5650437e-7,1.5650596e-7,1.5649854e-7,1.5650366e-7,1.565201e-7,1.5650348e-7,1.5653356e-7,1.5649971e-7,1.5649977e-7,1.5650299e-7,1.5650649e-7,1.5654891e-7,1.5673065e-7,1.5646461e-7,1.5650231e-7,1.565449e-7,1.5651078e-7,1.5651835e-7,1.5702419e-7,1.5650264e-7,1.5650049e-7,1.5649768e-7,1.5650222e-7,1.5650639e-7,1.5650735e-7,1.5535284e-7,1.5652466e-7,1.5650514e-7,1.565011e-7,1.5652313e-7,1.5758255e-7,1.5652387e-7,1.5379916e-7,1.5651675e-7,1.5650888e-7,1.5652053e-7,1.565044e-7,1.5650006e-7,1.5650133e-7,1.5649799e-7,1.5650654e-7,1.5651118e-7,1.5743866e-7,1.5651186e-7,1.564963e-7,1.5650193e-7,1.564957e-7,1.5649915e-7,1.5650602e-7,1.5650436e-7,1.5650645e-7,1.5650951e-7,1.5650315e-7,1.5650241e-7,1.5650356e-7,1.5412076e-7,1.5652031e-7,1.5649226e-7,1.5650255e-7,1.5676038e-7,1.5651749e-7,1.56504e-7,1.5651561e-7,1.5650353e-7,1.5650414e-7,1.5651484e-7,1.5652529e-7,1.5649563e-7,1.5650852e-7,1.565012e-7,1.5651452e-7,1.5651653e-7,1.5649555e-7,1.5460415e-7,1.5651145e-7,1.565046e-7,1.5651948e-7,1.565151e-7,1.5649661e-7,1.5650497e-7,1.565001e-7,1.5649846e-7,1.5668985e-7,1.5650163e-7,1.5650174e-7,1.5649697e-7,1.5650384e-7,1.5671414e-7,1.5650302e-7,1.5650296e-7,1.5651321e-7,1.565035e-7,1.5650355e-7,1.5650603e-7,1.5650149e-7,1.5650262e-7,1.5649763e-7,1.56514e-7,1.565189e-7,1.5651113e-7,1.5650264e-7,1.5650569e-7,1.5650197e-7,1.5649822e-7,1.5650852e-7,1.5650211e-7,1.565016e-7,1.5649994e-7,1.5650176e-7,1.5651491e-7,1.5650097e-7,1.5669023e-7,1.5650906e-7,1.56501e-7,1.5646087e-7,1.5649873e-7,1.5651231e-7,1.5650345e-7,1.5652307e-7,1.5415274e-7,1.5650123e-7,1.573143e-7,1.5650251e-7,1.5650893e-7,1.565009e-7,1.5650446e-7,1.5650285e-7,1.5649978e-7,1.5546323e-7,1.565088e-7,1.5651304e-7,1.5380829e-7,1.5649913e-7,1.5650475e-7,1.5651393e-7,1.5391167e-7,1.5650677e-7,1.5649975e-7,1.5651943e-7,1.5670825e-7,1.5649621e-7,1.5649765e-7,1.5650834e-7,1.5650176e-7,1.565015e-7,1.5650242e-7,1.5650274e-7,1.5650286e-7,1.5650562e-7,1.5652067e-7,1.5652266e-7,1.5679666e-7,1.5667419e-7,1.5652104e-7,1.5650583e-7,1.5453293e-7,1.5411118e-7,1.5650289e-7,1.5650959e-7,1.5651285e-7,1.5649778e-7,1.5651138e-7,1.565107e-7,1.5650353e-7,1.5651293e-7,1.5646766e-7,1.5651467e-7,1.5650768e-7,1.565109e-7,1.5650235e-7,1.5651345e-7,1.5650416e-7,1.5650446e-7,1.5441034e-7,1.5651563e-7,1.5649893e-7,1.5650146e-7,1.5649859e-7,1.5649846e-7,1.5651426e-7,1.5646792e-7,1.5650545e-7,1.5650288e-7,1.5650173e-7,1.5649876e-7,1.5650659e-7,1.5650117e-7,1.5650234e-7,1.5652135e-7,1.5650927e-7,1.5650875e-7,1.5650167e-7,1.5650282e-7,1.5652266e-7,1.5650934e-7,1.5650181e-7,1.5646401e-7,1.5650176e-7,1.5671739e-7,1.5650275e-7,1.5668502e-7,1.5650228e-7,1.5650112e-7,1.5650289e-7,1.564978e-7,1.5650268e-7,1.5649739e-7,1.56502e-7,1.5650623e-7,1.5693205e-7,1.565117e-7,1.5650308e-7,1.5650448e-7,1.565005e-7,1.5650593e-7,1.5651638e-7,1.5650213e-7,1.5650984e-7,1.5651031e-7,1.5650319e-7,1.5650552e-7,1.5650616e-7,1.5650276e-7,1.5650316e-7,1.5651234e-7,1.565213e-7,1.5654159e-7,1.565237e-7,1.5649856e-7,1.5651361e-7,1.5651527e-7,1.5647089e-7,1.5650377e-7,1.5650315e-7,1.5650123e-7,1.565085e-7,1.5650444e-7,1.5650564e-7,1.5646516e-7,1.5651302e-7,1.5650396e-7,1.5650444e-7,1.5650367e-7,1.5651011e-7,1.5649803e-7,1.5649981e-7,1.5646631e-7,1.5667342e-7,1.565029e-7,1.5650308e-7,1.5652667e-7,1.541929e-7,1.564994e-7,1.5650686e-7,1.5650431e-7,1.565076e-7,1.5649908e-7,1.5650642e-7,1.5649496e-7,1.5650214e-7,1.565449e-7,1.5650264e-7,1.5650231e-7,1.565234e-7,1.5649898e-7,1.565132e-7,1.5650424e-7,1.5666369e-7,1.5650383e-7,1.5649665e-7,1.5649653e-7,1.5650572e-7,1.5650723e-7,1.5649783e-7,1.5650345e-7,1.5650237e-7,1.565139e-7,1.5650082e-7,1.5649887e-7,1.5650308e-7,1.5650352e-7,1.539049e-7,1.5650032e-7,1.5650635e-7,1.5651433e-7,1.5651392e-7,1.5650939e-7,1.5405358e-7,1.5654906e-7,1.5650757e-7,1.5651304e-7,1.565097e-7,1.5649707e-7,1.5651487e-7,1.5650087e-7,1.5652317e-7,1.565035e-7,1.565155e-7,1.5651082e-7,1.5650234e-7,1.565025e-7,1.5650089e-7,1.5651251e-7,1.5649756e-7,1.5651838e-7,1.5649925e-7,1.5649617e-7,1.5650484e-7,1.5650015e-7,1.5649412e-7,1.5671556e-7,1.5650376e-7,1.5650575e-7,1.5650075e-7,1.5650839e-7,1.566904e-7,1.564979e-7,1.5650068e-7,1.5650483e-7,1.5674289e-7,1.5650267e-7,1.5649947e-7,1.5650406e-7,1.5650001e-7,1.5650825e-7,1.5650494e-7,1.5650421e-7,1.5650195e-7,1.5650215e-7,1.5651636e-7,1.5650528e-7,1.5649806e-7,1.5650375e-7,1.5652141e-7,1.5650807e-7,1.5650453e-7,1.565089e-7,1.5647025e-7,1.5650447e-7,1.5650606e-7,1.5650541e-7,1.543237e-7,1.5650467e-7,1.5649852e-7,1.5652266e-7,1.564971e-7,1.5651462e-7,1.5650271e-7,1.5650272e-7,1.5652415e-7,1.565008e-7,1.5650303e-7,1.5650235e-7,1.5649914e-7,1.5452814e-7,1.5650926e-7,1.5650338e-7,1.5650278e-7,1.5650929e-7,1.5651477e-7,1.5650089e-7,1.5651064e-7,1.5649975e-7,1.5650194e-7,1.5537047e-7,1.5739079e-7,1.5549678e-7,1.5651457e-7,1.5649871e-7,1.5650446e-7,1.5651392e-7,1.5651725e-7,1.5650585e-7,1.5650888e-7,1.5651537e-7,1.5650485e-7,1.5650738e-7,1.5650579e-7,1.5649428e-7,1.5650352e-7,1.5650308e-7,1.5651916e-7,1.5649975e-7,1.5649884e-7,1.5650392e-7,1.5650174e-7,1.5652337e-7,1.5652667e-7,1.5388667e-7,1.5646845e-7,1.565093e-7,1.565037e-7,1.5651038e-7,1.565031e-7,1.5650402e-7,1.565042e-7,1.5653205e-7,1.5650518e-7,1.5650329e-7,1.5651887e-7,1.5652276e-7,1.5646721e-7,1.5649734e-7,1.5649944e-7,1.5651368e-7,1.5650133e-7,1.5650105e-7,1.5650284e-7,1.565144e-7,1.564982e-7,1.5650214e-7,1.5650227e-7,1.5649825e-7,1.5414331e-7,1.5649866e-7,1.565017e-7,1.5650393e-7,1.5650326e-7,1.5651028e-7,1.565062e-7,1.5650572e-7,1.5649428e-7,1.5385474e-7,1.5649667e-7,1.5650592e-7,1.5651011e-7,1.565058e-7,1.5649653e-7,1.5650716e-7,1.5654892e-7,1.5651143e-7,1.565139e-7,1.5650585e-7,1.5650258e-7,1.5650247e-7,1.5650092e-7,1.565035e-7,1.5650107e-7,1.565063e-7,1.539119e-7,1.56502e-7,1.565193e-7,1.5650642e-7,1.5647241e-7,1.5649714e-7,1.5650244e-7,1.5649947e-7,1.565023e-7,1.5654896e-7,1.5651794e-7,1.565136e-7,1.5650477e-7,1.5651925e-7,1.5650195e-7,1.5649873e-7,1.564942e-7,1.5650232e-7,1.565034e-7,1.5651536e-7,1.5408818e-7,1.564986e-7,1.5650319e-7,1.5650315e-7,1.5650737e-7,1.5651672e-7,1.56504e-7,1.5651186e-7,1.5650348e-7,1.5649994e-7,1.5652294e-7,1.5649873e-7,1.5652164e-7,1.5650605e-7,1.5650234e-7,1.5650178e-7,1.5649738e-7,1.5650711e-7,1.5650178e-7,1.5651902e-7,1.5650397e-7,1.565194e-7,1.5652027e-7,1.5650586e-7,1.5369058e-7,1.5650048e-7,1.5650136e-7,1.5650363e-7,1.5650839e-7,1.5561665e-7,1.5651113e-7,1.5651189e-7,1.5650875e-7,1.5650384e-7,1.5646795e-7,1.5652077e-7,1.5650883e-7,1.565017e-7,1.5650053e-7,1.5649526e-7,1.5650579e-7,1.5650234e-7,1.565026e-7,1.5650959e-7,1.5649795e-7,1.565226e-7,1.5650744e-7,1.5650176e-7,1.5650294e-7,1.5650092e-7,1.5650436e-7,1.5650157e-7,1.5650869e-7,1.5649762e-7,1.5650375e-7,1.5650494e-7,1.564985e-7,1.5650672e-7,1.565141e-7,1.564999e-7,1.5667837e-7,1.565054e-7,1.564983e-7,1.5650186e-7,1.5650224e-7,1.5676848e-7,1.5669187e-7,1.564978e-7,1.5681034e-7,1.5650171e-7,1.5650436e-7,1.5652383e-7,1.5650004e-7,1.5650319e-7,1.5650727e-7,1.5649397e-7,1.5652736e-7,1.565146e-7,1.5522687e-7,1.5651634e-7,1.5651956e-7,1.5650171e-7,1.5650103e-7,1.5650235e-7,1.5651281e-7,1.5650284e-7,1.5647198e-7,1.5650197e-7,1.5651008e-7,1.5650416e-7,1.570459e-7,1.5650642e-7,1.5649704e-7,1.5649732e-7,1.5650406e-7,1.5650251e-7,1.5649721e-7,1.5650232e-7,1.5650183e-7,1.5650656e-7,1.565029e-7,1.567716e-7,1.565422e-7,1.5652638e-7,1.5651145e-7,1.5649893e-7,1.5665792e-7,1.5651023e-7,1.5667538e-7,1.5650949e-7,1.5654891e-7,1.5650537e-7,1.5650437e-7,1.5412076e-7,1.5651085e-7,1.5650227e-7,1.5650677e-7,1.5651729e-7,1.5649623e-7,1.565134e-7,1.5650325e-7,1.5652078e-7,1.5650014e-7,1.5651491e-7,1.564957e-7,1.5426397e-7,1.5646113e-7,1.5650879e-7,1.5650224e-7,1.5651399e-7,1.5650281e-7,1.565089e-7,1.5652225e-7,1.5650241e-7,1.565081e-7,1.5687196e-7,1.5650957e-7,1.5650232e-7,1.5650573e-7,1.5649742e-7,1.56514e-7,1.5652472e-7,1.5650497e-7,1.5649776e-7,1.5678592e-7,1.5432063e-7,1.5650465e-7,1.5650443e-7,1.5650319e-7,1.5650359e-7,1.5652003e-7,1.5650299e-7,1.5650126e-7,1.5673383e-7,1.5649928e-7,1.5650468e-7,1.5649476e-7,1.565134e-7,1.565036e-7,1.5650349e-7,1.5650517e-7,1.5649691e-7,1.5650144e-7,1.5650294e-7,1.5651347e-7,1.5649947e-7,1.5651985e-7,1.564986e-7,1.5651595e-7,1.5649832e-7,1.5651653e-7,1.565051e-7,1.5650853e-7,1.5650222e-7,1.5650161e-7,1.5649918e-7,1.5655098e-7,1.5650393e-7,1.5540395e-7,1.5652216e-7,1.5380898e-7,1.5652608e-7,1.5650173e-7,1.5651268e-7,1.5649452e-7,1.5650495e-7,1.5717308e-7,1.5650112e-7,1.5650183e-7,1.565094e-7,1.5651634e-7,1.5654372e-7,1.5651713e-7,1.5650336e-7,1.5651482e-7,1.5652195e-7,1.5652596e-7,1.5650372e-7,1.5669842e-7,1.5650298e-7,1.564955e-7,1.5651433e-7,1.5650306e-7,1.5650568e-7,1.5650964e-7,1.5650502e-7,1.564976e-7,1.5650315e-7,1.5650285e-7,1.5650727e-7,1.5650895e-7,1.565014e-7,1.5669774e-7,1.5652041e-7,1.5701386e-7,1.5650262e-7,1.5650926e-7,1.5649768e-7,1.541616e-7,1.5649829e-7,1.5650232e-7,1.5650305e-7,1.5650416e-7,1.5650403e-7,1.5650197e-7,1.5667794e-7,1.5654479e-7,1.5650501e-7,1.5652017e-7,1.5439495e-7,1.5650151e-7,1.5651074e-7,1.567517e-7,1.565035e-7,1.5649836e-7,1.5650483e-7,1.565089e-7,1.5650534e-7,1.5650444e-7,1.5650262e-7,1.5652331e-7,1.5650613e-7,1.5650176e-7,1.5650107e-7,1.5650484e-7,1.5650313e-7,1.5702223e-7,1.5650363e-7,1.5650608e-7,1.565025e-7,1.5650508e-7,1.5650608e-7,1.5654673e-7,1.5650068e-7,1.5649692e-7,1.5650181e-7,1.5683587e-7,1.5650852e-7,1.5651644e-7,1.5650053e-7,1.5650151e-7,1.565209e-7,1.5652124e-7,1.565222e-7,1.5652094e-7,1.565052e-7,1.5646843e-7,1.5652316e-7,1.5650251e-7,1.5418637e-7,1.5650153e-7,1.5651065e-7,1.5650666e-7,1.5651179e-7,1.5650053e-7,1.5650397e-7,1.5650781e-7,1.5649773e-7,1.5649675e-7,1.5651405e-7,1.5668586e-7,1.5651166e-7,1.5651497e-7,1.5650996e-7,1.5650207e-7,1.5651075e-7,1.5649563e-7,1.5650329e-7,1.5649621e-7,1.5649812e-7,1.5650214e-7,1.5652016e-7,1.5650105e-7,1.5650338e-7,1.5649924e-7,1.564957e-7,1.5652606e-7,1.5651135e-7,1.5651206e-7,1.5651564e-7,1.5649816e-7,1.5650731e-7,1.5650537e-7,1.5652589e-7,1.5650406e-7,1.565053e-7,1.5650258e-7,1.5650295e-7,1.5650167e-7,1.5650208e-7,1.5649633e-7,1.5651683e-7,1.5650136e-7,1.5652432e-7,1.5650588e-7,1.5649684e-7,1.5689136e-7,1.5650207e-7,1.5650603e-7,1.5649535e-7,1.565067e-7,1.5649582e-7,1.5650576e-7,1.5651322e-7,1.5650247e-7,1.5649684e-7,1.5650134e-7,1.5650684e-7,1.5649991e-7,1.5647214e-7,1.564964e-7,1.5651463e-7,1.5649933e-7,1.5651177e-7,1.5654669e-7,1.5650241e-7,1.565031e-7,1.5650481e-7,1.5422025e-7,1.5650383e-7,1.5650201e-7,1.5650095e-7,1.5649769e-7,1.565128e-7,1.565146e-7,1.5647558e-7,1.5650649e-7,1.56505e-7,1.564933e-7,1.5649655e-7,1.565235e-7,1.5649978e-7,1.5651456e-7,1.5651679e-7,1.565058e-7,1.5650039e-7,1.5650079e-7,1.5652519e-7,1.5650419e-7,1.565053e-7,1.5650464e-7,1.5650339e-7,1.5650042e-7,1.5649854e-7,1.5650546e-7,1.5646883e-7,1.5650326e-7,1.5650738e-7,1.565199e-7,1.5669774e-7,1.5364621e-7,1.5650122e-7,1.5649981e-7,1.5650488e-7,1.5649533e-7,1.5652593e-7,1.565148e-7,1.5651459e-7,1.5651173e-7,1.5652026e-7,1.5649877e-7,1.5651875e-7,1.5650087e-7,1.5649904e-7,1.5671345e-7,1.565054e-7,1.5650413e-7,1.5652328e-7,1.5650966e-7,1.5650207e-7,1.5650839e-7,1.5649685e-7,1.565043e-7,1.565029e-7,1.5649819e-7,1.5650328e-7,1.5651203e-7,1.5650191e-7,1.5398801e-7,1.5649975e-7,1.571516e-7,1.5651227e-7,1.5650285e-7,1.5650208e-7,1.5649196e-7,1.5649778e-7,1.5649947e-7,1.5650413e-7,1.564974e-7,1.564978e-7,1.5650004e-7,1.5650573e-7,1.5650551e-7,1.5650268e-7,1.5650312e-7,1.565058e-7,1.5650288e-7,1.5649933e-7,1.5650133e-7,1.5650305e-7,1.5651194e-7,1.5443696e-7,1.5650812e-7,1.5651375e-7,1.5650424e-7,1.5649539e-7,1.5650198e-7,1.5650495e-7,1.5650474e-7,1.5650538e-7,1.5650552e-7,1.5649317e-7,1.5452524e-7,1.5651835e-7,1.5650474e-7,1.5669224e-7,1.5651176e-7,1.5650015e-7,1.5674395e-7,1.5649958e-7,1.5650373e-7,1.565418e-7,1.5650009e-7,1.565053e-7,1.5650035e-7,1.5649756e-7,1.5650676e-7,1.5650261e-7,1.5650379e-7,1.5650717e-7,1.5649339e-7,1.5650093e-7,1.5647089e-7,1.5650508e-7,1.5669794e-7,1.5650289e-7,1.564956e-7,1.5438691e-7,1.5650237e-7,1.5651364e-7,1.5654275e-7,1.5650333e-7,1.56503e-7,1.5650919e-7,1.5651551e-7,1.5650144e-7,1.5649795e-7,1.5652633e-7,1.5439495e-7,1.5651665e-7,1.5650346e-7,1.5649807e-7,1.5650429e-7,1.5650551e-7,1.5650846e-7,1.5650144e-7,1.565035e-7,1.5652242e-7,1.565199e-7,1.5652314e-7,1.5650568e-7,1.5650564e-7,1.564975e-7,1.5417983e-7,1.565018e-7,1.565009e-7,1.5650505e-7,1.5649739e-7,1.5650478e-7,1.5649844e-7,1.564971e-7,1.5652049e-7,1.5651264e-7,1.5651969e-7,1.5651341e-7,1.5652276e-7,1.5650245e-7,1.5650426e-7,1.5649528e-7,1.5651148e-7,1.5652124e-7,1.5651406e-7,1.5649273e-7,1.5650265e-7,1.5650322e-7,1.5650309e-7,1.5650872e-7,1.5649671e-7,1.5650359e-7,1.5650102e-7,1.5768212e-7,1.5654832e-7,1.5649937e-7,1.5650286e-7,1.5650093e-7,1.5447766e-7,1.5651115e-7,1.5652658e-7,1.5650092e-7,1.5650845e-7,1.5653592e-7,1.5651449e-7,1.5650876e-7,1.5649495e-7,1.5649884e-7,1.5651152e-7,1.5651291e-7,1.5650649e-7,1.5650532e-7,1.565242e-7,1.565019e-7,1.5651152e-7,1.5650598e-7,1.5650905e-7,1.5650426e-7,1.5650436e-7,1.5650167e-7,1.5670118e-7,1.5650123e-7,1.5649974e-7,1.5650019e-7,1.5650942e-7,1.5650924e-7,1.5650099e-7,1.5649978e-7,1.5650983e-7,1.5650109e-7,1.5650275e-7,1.5651113e-7,1.5655652e-7,1.5650285e-7,1.5650666e-7,1.5650937e-7,1.5651314e-7,1.5650245e-7,1.5651395e-7,1.5650244e-7,1.5399912e-7,1.5649681e-7,1.5650247e-7,1.564966e-7,1.5650816e-7,1.5674053e-7,1.5650167e-7,1.565113e-7,1.5649935e-7,1.565037e-7,1.5650073e-7,1.5649658e-7,1.5678472e-7,1.5649992e-7,1.5649765e-7,1.5650338e-7,1.5671291e-7,1.5650666e-7,1.5677335e-7,1.5650747e-7,1.5650258e-7,1.5651061e-7,1.5650659e-7,1.5649681e-7,1.5650238e-7,1.5651236e-7,1.5651152e-7,1.5651214e-7,1.5650363e-7,1.5651736e-7,1.5651285e-7,1.5652293e-7,1.5457962e-7,1.5649789e-7,1.5650309e-7,1.56504e-7,1.5652688e-7,1.5651358e-7,1.5651284e-7,1.5683891e-7,1.5650372e-7,1.5650662e-7,1.5396219e-7,1.5651592e-7,1.5649812e-7,1.5650008e-7,1.5546966e-7,1.5651663e-7,1.5542587e-7,1.5649779e-7,1.5650907e-7,1.5651398e-7,1.5650181e-7,1.5717308e-7,1.5651518e-7,1.5650626e-7,1.5649853e-7,1.5650579e-7,1.5650274e-7,1.565302e-7],\"xaxis\":\"x\",\"y\":[-1.1139679e-8,-1.11705525e-8,-1.1135613e-8,-1.1151297e-8,-1.1133409e-8,-1.1191481e-8,-1.1144074e-8,-1.1158488e-8,-1.1171897e-8,-1.1170804e-8,-1.1018601e-8,-1.11324e-8,-1.1174103e-8,-1.1190162e-8,-1.1170903e-8,-1.1141198e-8,-1.1108951e-8,-1.1198685e-8,-1.1133516e-8,-1.114463e-8,-1.1091752e-8,-1.1139922e-8,-1.1141968e-8,-1.1177293e-8,-1.1142187e-8,-1.1141287e-8,-1.1137554e-8,-1.1166817e-8,-1.1175368e-8,-1.1130689e-8,-1.1199168e-8,-1.1144807e-8,-1.1171942e-8,-1.1184865e-8,-1.1030469e-8,-1.1164418e-8,-1.1140716e-8,-1.1143161e-8,-1.1134177e-8,-1.1236896e-8,-1.1139635e-8,-1.1190206e-8,-1.113171e-8,-1.1189539e-8,-1.09250875e-8,-1.1171935e-8,-1.1140823e-8,-1.1199649e-8,-1.1179293e-8,-1.1055878e-8,-1.1134221e-8,-1.1135783e-8,-1.1139773e-8,-1.1032842e-8,-1.1091179e-8,-1.1121023e-8,-1.1137506e-8,-1.1144887e-8,-1.1142593e-8,-1.1188116e-8,-1.1108983e-8,-1.1114955e-8,-1.1173395e-8,-1.1138699e-8,-1.109844e-8,-1.1173997e-8,-1.11341265e-8,-1.1174752e-8,-1.11704574e-8,-1.1177611e-8,-1.1172392e-8,-1.1170717e-8,-1.11659935e-8,-1.11401155e-8,-1.1172348e-8,-1.117286e-8,-1.1175277e-8,-1.1141961e-8,-1.11726735e-8,-1.1131487e-8,-1.1171207e-8,-1.1103108e-8,-1.1173525e-8,-1.11762075e-8,-1.1175356e-8,-1.1169723e-8,-1.11884155e-8,-1.1138778e-8,-1.1092292e-8,-1.11765805e-8,-1.113466e-8,-1.1164893e-8,-1.1173986e-8,-1.1198176e-8,-1.1173403e-8,-1.1140931e-8,-1.1184937e-8,-1.1176645e-8,-1.1153867e-8,-1.1144188e-8,-1.1174791e-8,-1.1173542e-8,-1.1179979e-8,-1.1134862e-8,-1.1173257e-8,-1.1131608e-8,-1.1177152e-8,-1.1170886e-8,-1.1173831e-8,-1.1017733e-8,-1.1143384e-8,-1.1167006e-8,-1.11316485e-8,-1.1198773e-8,-1.1171973e-8,-1.114177e-8,-1.1095914e-8,-1.1176101e-8,-1.1173771e-8,-1.1183814e-8,-1.1173624e-8,-1.1168609e-8,-1.11650955e-8,-1.1029242e-8,-1.1151441e-8,-1.1141357e-8,-1.11707035e-8,-1.1145287e-8,-1.1141864e-8,-1.1143448e-8,-1.1138954e-8,-1.11382e-8,-1.11513065e-8,-1.1172396e-8,-1.1173438e-8,-1.1190233e-8,-1.1173961e-8,-1.1089594e-8,-1.1091197e-8,-1.1173521e-8,-1.1166907e-8,-1.1142821e-8,-1.1153148e-8,-1.1143574e-8,-1.1088707e-8,-1.1132374e-8,-1.0858771e-8,-1.1157172e-8,-1.1141004e-8,-1.1176908e-8,-1.1171863e-8,-1.1171963e-8,-1.1141966e-8,-1.1142219e-8,-1.1170916e-8,-1.11439045e-8,-1.1138927e-8,-1.11401315e-8,-1.1089366e-8,-1.113832e-8,-1.11529985e-8,-1.1172665e-8,-1.1137679e-8,-1.11264935e-8,-1.0940828e-8,-1.116381e-8,-1.11739356e-8,-1.11686855e-8,-1.1141233e-8,-1.119022e-8,-1.1133443e-8,-1.109129e-8,-1.1090661e-8,-1.1170627e-8,-1.1139172e-8,-1.1164648e-8,-1.1199146e-8,-1.1140272e-8,-1.1171127e-8,-1.1142577e-8,-1.1091315e-8,-1.1135905e-8,-1.1189681e-8,-1.1166713e-8,-1.1143338e-8,-1.1145098e-8,-1.109022e-8,-1.109179e-8,-1.1139143e-8,-1.1144094e-8,-1.1129929e-8,-1.1166022e-8,-1.1152491e-8,-1.1088551e-8,-1.11890275e-8,-1.1135987e-8,-1.1091713e-8,-1.1141928e-8,-1.1153554e-8,-1.1143354e-8,-1.1169883e-8,-1.1183967e-8,-1.1141732e-8,-1.11753415e-8,-1.1131265e-8,-1.1106061e-8,-1.1131922e-8,-1.1181781e-8,-1.1188196e-8,-1.1173056e-8,-1.0862264e-8,-1.1133452e-8,-1.1172883e-8,-1.11759775e-8,-1.1175788e-8,-1.1131909e-8,-1.11648735e-8,-1.1176458e-8,-1.1165999e-8,-1.110699e-8,-1.1177929e-8,-1.1164925e-8,-1.1027135e-8,-1.114182e-8,-1.1145793e-8,-1.11432845e-8,-1.1030949e-8,-1.1135991e-8,-1.1187989e-8,-1.11434275e-8,-1.1141291e-8,-1.1189624e-8,-1.1172024e-8,-1.1135254e-8,-1.11999094e-8,-1.116524e-8,-1.115332e-8,-1.1140426e-8,-1.1198218e-8,-1.1172607e-8,-1.1141574e-8,-1.1144247e-8,-1.1167401e-8,-1.1174236e-8,-1.1127304e-8,-1.1152947e-8,-1.0913683e-8,-1.0866066e-8,-1.1172191e-8,-1.1131908e-8,-1.1139581e-8,-1.118981e-8,-1.1141999e-8,-1.11434595e-8,-1.1174922e-8,-1.1164461e-8,-1.11405205e-8,-1.1125055e-8,-1.1145827e-8,-1.1141192e-8,-1.1190202e-8,-1.1140031e-8,-1.115765e-8,-1.1153431e-8,-1.0896036e-8,-1.1170928e-8,-1.11936185e-8,-1.1144868e-8,-1.1198456e-8,-1.1090413e-8,-1.1140841e-8,-1.1174259e-8,-1.1165987e-8,-1.1090872e-8,-1.1143708e-8,-1.1141712e-8,-1.1140771e-8,-1.1144541e-8,-1.1171201e-8,-1.117171e-8,-1.116346e-8,-1.1170013e-8,-1.116475e-8,-1.1091864e-8,-1.1175446e-8,-1.1140299e-8,-1.11726735e-8,-1.1108816e-8,-1.1143323e-8,-1.1139994e-8,-1.1143687e-8,-1.1164896e-8,-1.1173332e-8,-1.11650404e-8,-1.117403e-8,-1.117479e-8,-1.1176381e-8,-1.1198891e-8,-1.1188184e-8,-1.1174894e-8,-1.1123176e-8,-1.1142835e-8,-1.1197585e-8,-1.1116034e-8,-1.1138166e-8,-1.1172874e-8,-1.1134435e-8,-1.1145614e-8,-1.1156985e-8,-1.117256e-8,-1.1197932e-8,-1.11752145e-8,-1.1174051e-8,-1.1143182e-8,-1.1171056e-8,-1.1141242e-8,-1.1152939e-8,-1.116747e-8,-1.1153158e-8,-1.1198348e-8,-1.117296e-8,-1.1170332e-8,-1.1139732e-8,-1.1092152e-8,-1.1153182e-8,-1.1088429e-8,-1.1138379e-8,-1.11292255e-8,-1.1144153e-8,-1.1138839e-8,-1.1175152e-8,-1.1143502e-8,-1.11388765e-8,-1.1140561e-8,-1.1121182e-8,-1.1198493e-8,-1.1173541e-8,-1.1099232e-8,-1.1166628e-8,-1.1172041e-8,-1.1133094e-8,-1.1142369e-8,-1.08747535e-8,-1.1140566e-8,-1.1170718e-8,-1.1140161e-8,-1.117185e-8,-1.11978995e-8,-1.113401e-8,-1.1131121e-8,-1.1171065e-8,-1.1177152e-8,-1.1166022e-8,-1.1089287e-8,-1.1142379e-8,-1.1090241e-8,-1.11753184e-8,-1.1166436e-8,-1.1143503e-8,-1.1142265e-8,-1.1131438e-8,-1.1198749e-8,-1.1131072e-8,-1.1177963e-8,-1.1090009e-8,-1.113457e-8,-1.1173196e-8,-1.1172871e-8,-1.11787335e-8,-1.1184602e-8,-1.1139991e-8,-1.1184812e-8,-1.1031322e-8,-1.1136546e-8,-1.1143427e-8,-1.1136302e-8,-1.1165329e-8,-1.11645315e-8,-1.1039787e-8,-1.114409e-8,-1.1170607e-8,-1.1171892e-8,-1.1137895e-8,-1.11985985e-8,-1.1171145e-8,-1.1170007e-8,-1.112937e-8,-1.1134932e-8,-1.11695595e-8,-1.1141165e-8,-1.1187776e-8,-1.114198e-8,-1.1152067e-8,-1.1127919e-8,-1.118889e-8,-1.11337295e-8,-1.1169129e-8,-1.1150393e-8,-1.1144975e-8,-1.1180807e-8,-1.1115821e-8,-1.1133882e-8,-1.1091512e-8,-1.1176706e-8,-1.1091281e-8,-1.11797345e-8,-1.1142491e-8,-1.11871845e-8,-1.1179947e-8,-1.1166403e-8,-1.11398055e-8,-1.1089286e-8,-1.1173677e-8,-1.1140953e-8,-1.1128744e-8,-1.1120858e-8,-1.11343565e-8,-1.1143051e-8,-1.1198835e-8,-1.1131691e-8,-1.1142401e-8,-1.1166265e-8,-1.1138073e-8,-1.1090644e-8,-1.1140814e-8,-1.1111367e-8,-1.1142299e-8,-1.1137099e-8,-1.11094804e-8,-1.1171104e-8,-1.1172801e-8,-1.11666365e-8,-1.08906475e-8,-1.1178518e-8,-1.1137381e-8,-1.1141732e-8,-1.1131122e-8,-1.1137767e-8,-1.1190466e-8,-1.1134429e-8,-1.1143177e-8,-1.1174998e-8,-1.1188155e-8,-1.1198112e-8,-1.1131106e-8,-1.09108695e-8,-1.1141238e-8,-1.1169957e-8,-1.1139411e-8,-1.1165332e-8,-1.1163896e-8,-1.113605e-8,-1.114199e-8,-1.1091308e-8,-1.1188868e-8,-1.09758425e-8,-1.11791465e-8,-1.1079335e-8,-1.1179595e-8,-1.1088507e-8,-1.1173075e-8,-1.1166698e-8,-1.1141306e-8,-1.1167304e-8,-1.1140753e-8,-1.1143849e-8,-1.1125857e-8,-1.1174214e-8,-1.1170169e-8,-1.1130564e-8,-1.1175299e-8,-1.1175174e-8,-1.1141082e-8,-1.1169499e-8,-1.1182472e-8,-1.1165799e-8,-1.1175897e-8,-1.1173904e-8,-1.1143217e-8,-1.10302e-8,-1.1138416e-8,-1.1163461e-8,-1.1183278e-8,-1.1142363e-8,-1.1141073e-8,-1.1182114e-8,-1.1188337e-8,-1.1144232e-8,-1.1153088e-8,-1.109159e-8,-1.114065e-8,-1.1144383e-8,-1.1131563e-8,-1.11897736e-8,-1.1143168e-8,-1.1174496e-8,-1.1131495e-8,-1.1167023e-8,-1.119031e-8,-1.1138741e-8,-1.11316245e-8,-1.11982015e-8,-1.1169902e-8,-1.1092126e-8,-1.1040627e-8,-1.1184437e-8,-1.1199118e-8,-1.1189088e-8,-1.1141026e-8,-1.1142589e-8,-1.1171773e-8,-1.1139319e-8,-1.1172662e-8,-1.1030365e-8,-1.1130489e-8,-1.1171806e-8,-1.1170006e-8,-1.1143517e-8,-1.1190595e-8,-1.1177788e-8,-1.1137896e-8,-1.1142346e-8,-1.1141251e-8,-1.1129711e-8,-1.1198534e-8,-1.11418235e-8,-1.1192315e-8,-1.11336425e-8,-1.1141109e-8,-1.11433875e-8,-1.102981e-8,-1.1198891e-8,-1.1141243e-8,-1.1139191e-8,-1.114223e-8,-1.1189131e-8,-1.1089879e-8,-1.1189021e-8,-1.11726495e-8,-1.1139872e-8,-1.1174288e-8,-1.1152623e-8,-1.1145776e-8,-1.1152082e-8,-1.1175536e-8,-1.1131922e-8,-1.1140357e-8,-1.1170827e-8,-1.1140211e-8,-1.1172354e-8,-1.0861652e-8,-1.1189033e-8,-1.11975735e-8,-1.1173543e-8,-1.1132931e-8,-1.11314895e-8,-1.1166738e-8,-1.1136081e-8,-1.1089126e-8,-1.1091845e-8,-1.1134398e-8,-1.1197291e-8,-1.1178002e-8,-1.1139936e-8,-1.1139653e-8,-1.114112e-8,-1.1089378e-8,-1.1146059e-8,-1.1142594e-8,-1.1180318e-8,-1.1138562e-8,-1.1144285e-8,-1.1175373e-8,-1.1143627e-8,-1.1020952e-8,-1.117543e-8,-1.11317995e-8,-1.1086347e-8,-1.1143035e-8,-1.11091145e-8,-1.1164488e-8,-1.1138427e-8,-1.1165886e-8,-1.1175665e-8,-1.1167013e-8,-1.1172533e-8,-1.1142755e-8,-1.1188092e-8,-1.1165901e-8,-1.1090061e-8,-1.1137846e-8,-1.1190475e-8,-1.1114436e-8,-1.11517e-8,-1.1144625e-8,-1.1142141e-8,-1.1141404e-8,-1.1144602e-8,-1.1143361e-8,-1.108913e-8,-1.1166907e-8,-1.11423315e-8,-1.1164755e-8,-1.1189675e-8,-1.1165978e-8,-1.1173733e-8,-1.10923235e-8,-1.1134973e-8,-1.1141952e-8,-1.1091038e-8,-1.11413e-8,-1.1145797e-8,-1.1184067e-8,-1.1182748e-8,-1.11886305e-8,-1.11712115e-8,-1.1140087e-8,-1.1092167e-8,-1.1123105e-8,-1.1190699e-8,-1.1166907e-8,-1.110488e-8,-1.1139376e-8,-1.11744525e-8,-1.1143248e-8,-1.1163147e-8,-1.1133462e-8,-1.11414264e-8,-1.0958359e-8,-1.1141394e-8,-1.11713945e-8,-1.1125484e-8,-1.1192338e-8,-1.1198112e-8,-1.1172953e-8,-1.1169186e-8,-1.1135864e-8,-1.11989635e-8,-1.1138384e-8,-1.1175533e-8,-1.1138853e-8,-1.1177165e-8,-1.1189456e-8,-1.1138151e-8,-1.1154117e-8,-1.11421485e-8,-1.1163532e-8,-1.1190473e-8,-1.1140489e-8,-1.11433955e-8,-1.1154853e-8,-1.1120422e-8,-1.1142179e-8,-1.1172742e-8,-1.1139898e-8,-1.1197907e-8,-1.11286855e-8,-1.11706795e-8,-1.1145127e-8,-1.117128e-8,-1.1174414e-8,-1.1153443e-8,-1.1188544e-8,-1.0858771e-8,-1.11701945e-8,-1.1088455e-8,-1.1142145e-8,-1.1133908e-8,-1.1089722e-8,-1.1168985e-8,-1.1089965e-8,-1.1171342e-8,-1.114105e-8,-1.1144061e-8,-1.1090248e-8,-1.0887699e-8,-1.111569e-8,-1.1141537e-8,-1.11886305e-8,-1.1129663e-8,-1.1171544e-8,-1.1099159e-8,-1.1142428e-8,-1.1143306e-8,-1.1173443e-8,-1.1126917e-8,-1.1150825e-8,-1.1171325e-8,-1.1143407e-8,-1.1129833e-8,-1.1165528e-8,-1.11422125e-8,-1.1141928e-8,-1.1198582e-8,-1.1064433e-8,-1.0920882e-8,-1.1166695e-8,-1.1166226e-8,-1.1184841e-8,-1.1145192e-8,-1.1141706e-8,-1.1141524e-8,-1.1201089e-8,-1.11756675e-8,-1.10901155e-8,-1.1141365e-8,-1.1198845e-8,-1.1165324e-8,-1.1172682e-8,-1.1139452e-8,-1.11414815e-8,-1.1089659e-8,-1.1130747e-8,-1.114135e-8,-1.117304e-8,-1.1091963e-8,-1.1108794e-8,-1.1184338e-8,-1.1126114e-8,-1.11309095e-8,-1.1171864e-8,-1.1166534e-8,-1.1141667e-8,-1.1090169e-8,-1.1191694e-8,-1.1173061e-8,-1.1138117e-8,-1.1092624e-8,-1.109901e-8,-1.11649925e-8,-1.1027042e-8,-1.1144791e-8,-1.1088361e-8,-1.1138511e-8,-1.1199453e-8,-1.1142149e-8,-1.1173947e-8,-1.1171445e-8,-1.1139018e-8,-1.1165685e-8,-1.11710134e-8,-1.1187299e-8,-1.117753e-8,-1.1173916e-8,-1.11720375e-8,-1.1140101e-8,-1.1134821e-8,-1.1170853e-8,-1.1142202e-8,-1.1184125e-8,-1.1091691e-8,-1.114297e-8,-1.1172661e-8,-1.11663105e-8,-1.1141837e-8,-1.1141381e-8,-1.1184139e-8,-1.114298e-8,-1.1090199e-8,-1.11432525e-8,-1.1144958e-8,-1.1187989e-8,-1.1172641e-8,-1.1168919e-8,-1.1174902e-8,-1.1089504e-8,-1.11698775e-8,-1.1198585e-8,-1.0855418e-8,-1.1143734e-8,-1.1141474e-8,-1.1171944e-8,-1.1176886e-8,-1.1144717e-8,-1.11410206e-8,-1.1171922e-8,-1.1167474e-8,-1.1143232e-8,-1.11762875e-8,-1.0895885e-8,-1.1187704e-8,-1.1142033e-8,-1.1175503e-8,-1.1174705e-8,-1.11892575e-8,-1.11728315e-8,-1.1137099e-8,-1.11431655e-8,-1.1177614e-8,-1.109022e-8,-1.1165332e-8,-1.1142703e-8,-1.1089036e-8,-1.1165773e-8,-1.11738485e-8,-1.1177811e-8,-1.1128149e-8,-1.1143677e-8,-1.1145338e-8,-1.1175925e-8,-1.1114793e-8,-1.11444525e-8,-1.1173661e-8,-1.1141247e-8,-1.1120475e-8,-1.1186377e-8,-1.1134811e-8,-1.1174056e-8,-1.1141866e-8,-1.1197173e-8,-1.1126865e-8,-1.1120926e-8,-1.1140042e-8,-1.1164235e-8,-1.1133141e-8,-1.11542455e-8,-1.116382e-8,-1.1179259e-8,-1.1198732e-8,-1.0883153e-8,-1.1168361e-8,-1.1142705e-8,-1.1170677e-8,-1.1134868e-8,-1.11516e-8,-1.1176951e-8,-1.1141614e-8,-1.1188827e-8,-1.1089402e-8,-1.117555e-8,-1.1134146e-8,-1.1175275e-8,-1.1144126e-8,-1.1170115e-8,-1.1177107e-8,-1.1140254e-8,-1.1190722e-8,-1.11429745e-8,-1.1091043e-8,-1.1194462e-8,-1.11432295e-8,-1.1177189e-8,-1.1142768e-8,-1.11359695e-8,-1.1198713e-8,-1.1089594e-8,-1.1171663e-8,-1.1143783e-8,-1.1169918e-8,-1.1141459e-8,-1.1088519e-8,-1.1174232e-8,-1.115285e-8,-1.11441025e-8,-1.1146287e-8,-1.1142528e-8,-1.109239e-8,-1.114181e-8,-1.1144683e-8,-1.116965e-8,-1.1091346e-8,-1.11658585e-8,-1.11317995e-8,-1.1170665e-8,-1.11537934e-8,-1.1130314e-8,-1.1133818e-8,-1.114168e-8,-1.113215e-8,-1.1130731e-8,-1.1143966e-8,-1.1090008e-8,-1.1167602e-8,-1.11336504e-8,-1.1138181e-8,-1.1131924e-8,-1.117306e-8,-1.1173001e-8,-1.11718395e-8,-1.1170126e-8,-1.1118768e-8,-1.11719345e-8,-1.1139241e-8,-1.1141546e-8,-1.11778995e-8,-1.1171773e-8,-1.1188747e-8,-1.1141605e-8,-1.0868922e-8,-1.1167437e-8,-1.1170477e-8,-1.11712e-8,-1.1184544e-8,-1.1152615e-8,-1.1142647e-8,-1.1170112e-8,-1.11442695e-8,-1.11423e-8,-1.1178577e-8,-1.1187524e-8,-1.1141182e-8,-1.1167379e-8,-1.1140436e-8,-1.1138793e-8,-1.1131153e-8,-1.119263e-8,-1.1191705e-8,-1.1170669e-8,-1.1177548e-8,-1.1143637e-8,-1.1166242e-8,-1.1091837e-8,-1.1197669e-8,-1.1143645e-8,-1.1141636e-8,-1.1163687e-8,-1.1184761e-8,-1.1168685e-8,-1.1133429e-8,-1.1172641e-8,-1.10159935e-8,-1.1131673e-8,-1.1091293e-8,-1.1165008e-8,-1.108859e-8,-1.113535e-8,-1.1141388e-8,-1.1132981e-8,-1.1164396e-8,-1.1172109e-8,-1.1090093e-8,-1.11840555e-8,-1.1120586e-8,-1.1088074e-8,-1.1139662e-8,-1.1176333e-8,-1.116691e-8,-1.117222e-8,-1.1172937e-8,-1.1170126e-8,-1.11797345e-8,-1.1198854e-8,-1.1130272e-8,-1.1176681e-8,-1.1189578e-8,-1.1184869e-8,-1.114048e-8,-1.1200944e-8,-1.103343e-8,-1.1143181e-8,-1.1174881e-8,-1.1172158e-8,-1.1169019e-8,-1.1142866e-8,-1.1134477e-8,-1.118981e-8,-1.1183993e-8,-1.1172011e-8,-1.11709255e-8,-1.1092167e-8,-1.1139376e-8,-1.1178601e-8,-1.114195e-8,-1.1198441e-8,-1.1143255e-8,-1.1166599e-8,-1.114052e-8,-1.1163691e-8,-1.1131931e-8,-1.11727765e-8,-1.11739835e-8,-1.08916325e-8,-1.1141388e-8,-1.1133071e-8,-1.1165571e-8,-1.11989475e-8,-1.1166052e-8,-1.1139883e-8,-1.1142849e-8,-1.1140439e-8,-1.1171856e-8,-1.117012e-8,-1.0903647e-8,-1.1172388e-8,-1.1176878e-8,-1.1134746e-8,-1.11645795e-8,-1.1127945e-8,-1.1150545e-8,-1.11035785e-8,-1.1142851e-8,-1.1144893e-8,-1.11586225e-8,-1.1172646e-8,-1.1141641e-8,-1.113026e-8,-1.1142533e-8,-1.1174778e-8,-1.115309e-8,-1.1144724e-8,-1.1139489e-8,-1.11701945e-8,-1.1173594e-8,-1.1165524e-8,-1.1138438e-8,-1.1173599e-8,-1.1130387e-8,-1.0907877e-8,-1.1176246e-8,-1.1171099e-8,-1.11670975e-8,-1.116689e-8,-1.11850325e-8,-1.1177011e-8,-1.1172811e-8,-1.1126237e-8,-1.1184543e-8,-1.1172624e-8,-1.0895885e-8,-1.1173302e-8,-1.1136173e-8,-1.1184766e-8,-1.1187766e-8,-1.1176397e-8,-1.1172859e-8,-1.1088779e-8,-1.1144619e-8,-1.11718546e-8,-1.1109266e-8,-1.1142365e-8,-1.1143283e-8,-1.11411484e-8,-1.11985825e-8,-1.0870529e-8,-1.114046e-8,-1.1131909e-8,-1.1144579e-8,-1.1162446e-8,-1.116635e-8,-1.1172505e-8,-1.1128849e-8,-1.1164486e-8,-1.1172782e-8,-1.1172924e-8,-1.1171473e-8,-1.1183655e-8,-1.1130571e-8,-1.1183826e-8,-1.1199268e-8,-1.1143773e-8,-1.117663e-8,-1.1137824e-8,-1.1094959e-8,-1.1156674e-8,-1.1089022e-8,-1.1143786e-8,-1.1139594e-8,-1.11204965e-8,-1.1142767e-8,-1.1197953e-8,-1.1091039e-8,-1.1146458e-8,-1.1089529e-8,-1.1088275e-8,-1.1139515e-8,-1.0896042e-8,-1.11724106e-8,-1.1143071e-8,-1.1164599e-8,-1.1172071e-8,-1.1137567e-8,-1.1169393e-8,-1.1142309e-8,-1.1090783e-8,-1.1198591e-8,-1.114501e-8,-1.1141523e-8,-1.1143973e-8,-1.1170675e-8,-1.11411085e-8,-1.1137147e-8,-1.1143469e-8,-1.11431895e-8,-1.1173271e-8,-1.1140368e-8,-1.1141756e-8,-1.116475e-8,-1.1166774e-8,-1.11391785e-8,-1.1140296e-8,-1.1139704e-8,-1.1143597e-8,-1.1177193e-8,-1.1141524e-8,-1.11327765e-8,-1.1142445e-8,-1.1191768e-8,-1.1187948e-8,-1.1170515e-8,-1.1114604e-8,-1.1142227e-8,-1.1142917e-8,-1.1139125e-8,-1.1170811e-8,-1.1090316e-8,-1.1152158e-8,-1.1084569e-8,-1.1036306e-8,-1.1130164e-8,-1.1142634e-8,-1.1198604e-8,-1.1177706e-8,-1.1124157e-8,-1.11574785e-8,-1.1139421e-8,-1.1133029e-8,-1.1184354e-8,-1.1130198e-8,-1.1089422e-8,-1.1170017e-8,-1.115878e-8,-1.1144284e-8,-1.1178193e-8,-1.11700915e-8,-1.117568e-8,-1.1138986e-8,-1.11775345e-8,-1.1190091e-8,-1.1141289e-8,-1.1130949e-8,-1.1190181e-8,-1.11697736e-8,-1.117203e-8,-1.1164147e-8,-1.1135121e-8,-1.1091465e-8,-1.113295e-8,-1.1171304e-8,-1.1172598e-8,-1.0920136e-8,-1.1175936e-8,-1.1190055e-8,-1.1139016e-8,-1.1144943e-8,-1.1176321e-8,-1.1142053e-8,-1.116974e-8,-1.11395755e-8,-1.1166543e-8,-1.10309335e-8,-1.1171589e-8,-1.1092899e-8,-1.1168959e-8,-1.111478e-8,-1.114104e-8,-1.1242857e-8,-1.1088511e-8,-1.1142561e-8,-1.1171387e-8,-1.1090362e-8,-1.1173947e-8,-1.1135305e-8,-1.1140386e-8,-1.1175409e-8,-1.11352705e-8,-1.11735865e-8,-1.1178132e-8],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"label\"}},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"t-SNE Visualization of BERT Embeddings (perplexity=1166)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('abc79a3b-f585-442d-a2fd-39de3efb13f9');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization of embeddings for train set\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "\n",
    "sent_embeddings_train_2D = np.array([emb[0] for emb in sent_embeddings_train])\n",
    "tsne = TSNE(n_components=2,random_state=42,perplexity=(example_number-1))\n",
    "reduced_embeddings = tsne.fit_transform(sent_embeddings_train_2D)\n",
    "\n",
    "df = pd.DataFrame(reduced_embeddings, columns=['x', 'y'])\n",
    "df['label'] = labels_train  # При условии, что есть метки\n",
    "\n",
    "# Создание точечного графика\n",
    "fig = px.scatter(df, x='x', y='y', color='label', title=f't-SNE Visualization of BERT Embeddings (perplexity={(example_number-1)})')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "i5yLLij2rvbA",
    "outputId": "5dfd105d-3ab1-4eb5-fa40-2026cea05e3d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"832b090e-3ccb-4a9f-a824-9b483aafc8c3\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"832b090e-3ccb-4a9f-a824-9b483aafc8c3\")) {                    Plotly.newPlot(                        \"832b090e-3ccb-4a9f-a824-9b483aafc8c3\",                        [{\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003elabel=%{marker.color}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[0,0,0,-1,-1,0,-1,0,1,-1,1,1,1,-1,0,1,1,-1,0,1,-1,1,-1,0,0,-1,0,0,0,0,0,-1,1,1,0,1,1,1,1,-1,-1,-1,0,-1,0,0,1,-1,0,1,0,-1,-1,1,0,0,1,0,0,-1,1,0,-1,1,0,0,-1,0,0,0,-1,1,0,1,0,-1,1,0,1,0,0,0,0,0,0,1,1,0,1,0,0,0,1,1,1,1,1,1,1,0,-1,1,0,0,-1,-1,1,0,1,0,-1,0,0,1,0,1,1,1,-1,-1,-1,-1,0,1,-1,1,0,-1,0,0,0,1,0,1,0,1,-1,-1,1,-1,0,-1,0,0,1,0,-1,-1,0,0,1,0,1,-1,0,1,0,1,1,1,1,1,1,1,1,-1,0,-1,0,0,1,0,1,1,0,0,0,0,0,0,1,1,1,-1,0,-1,0,1,1,1,0,1,0,0,-1,-1,1,1,1,0,0,-1,-1,0,0,0,-1,0,1,0,1,0,-1,1,0,-1,0,0,1,0,-1,0,-1,-1,0,0,1,-1,1,1,-1,1,-1,0,0,1,1,1,-1,-1,1,1,0,-1,0,-1,1,1,0,0,0,-1,0,1,0,0,-1,1,-1,0,1,-1,0,1,1,1,0,1,1,-1,0,0,1,1,1,1,-1,1,1,1,0,1,1,1,0,1,1,0,0,0,0,0,-1,-1,-1,0,1,-1,0,1,1,-1,0,0,1,0,0,0,-1,1,1,0,1,0,0,-1,1,0,0,-1,-1,0,0,1,-1,1,0,-1,0,-1,1,-1,0,0,1,1,-1,1,-1,1,1,-1,1,1,1,-1,1,-1,1,-1,-1,1,1,-1,1,0,0,1,-1,1,0,0,1,1,0,-1,1,0,1,1,-1,-1,1,0,1,0,-1,1,1,0,-1,1,0,0,1,0,1,1,1,-1,1,-1,1,-1,0,0,0,0,-1,1,-1,0,0,0,0,1,-1,-1,-1,-1,1,1,1,-1,0,-1,1,0,0,0,0,-1,-1,0,1,1,0,1,1,1,1,0,0,-1,0,0,0,1,-1,0,0,-1,1,-1,-1,0,0,0,0,0,-1,0,0,0,0,-1,0,0,-1,0,1,1,1,1,-1,0,-1,1,0,0,1,1,1,-1,0,0,1,-1,-1,0,0,1,1,1,0,0,1,1,-1,1,0,1,-1,0,1,1,0,0,-1,1,0,-1,0,0,1,-1,1,1,1,1,-1,0,1,0,-1,0,0,1,1,1,0,0,-1,0,1,1,0,0,0,-1,-1,-1,1,0,0,1,0,1,0,1,0,1,0,0,0,-1,1,1,0,0,-1,0,1,0,-1,-1,0,0,-1,-1,0,1,0,0,0,1,-1,0,0,-1,0,0,0,-1,-1,1,0,0,-1,-1,1,0,1,-1,1,1,1,1,0,0,-1,1,-1,1,0,-1,0,-1,0,0,0,1,0,1,0,1,-1,-1,-1,-1,-1,0,1,1,0,1,1,-1,1,-1,0,0,0,1,0,0,1,0,0,1,1,0,1,1,-1,0,0,-1,0,-1,0,0,1,0,0,0,-1,-1,1,0,1,0,1,1,1,-1,1,-1,1,-1,-1,0,1,-1,0,0,0,1,0,1,1,0,-1,1,-1,-1,0,-1,1,1,-1,-1,-1,1,1,0,-1,1,0,0,-1,0,0,0,1,0,1,0,1,0,1,1,-1,-1,0,1,-1,0,0,1,0,0,1,1,1,-1,-1,1,0,0,0,0,1,0,0,0,0,-1,0,0,1,1,1,1,-1,-1,0,0,1,0,-1,1,-1,1,0,1,1,1,0,0,0,1,0,1,1,0,-1,1,1,-1,1,1,1,0,-1,1,0,1,-1,1,-1,1,1,1,1,-1,1,0,0,1,0,1,0,1,1,1,1,0,1,0,0,0,0,-1,1,0,-1,0,1,-1,0,1,-1,1,1,1,1,0,0,-1,1,0,0,1,1,1,-1,0,0,1,1,1,1,0,-1,0,1,-1,0,0,1,1,-1,0,0,1,0,1,-1,-1,0,1,0,1,-1,1,1,0,1,1,1,1,-1,-1,-1,-1,1,1,0,-1,1,-1,1,1,1,0,0,0,0,1,1,0,1,0,1,1,0,-1,0,-1,1,-1,1,-1,-1,-1,1,1,-1,1,0,1,1,1,-1,1,0,1,1,0,0,-1,-1,0,0,0,0,0,1,1,0,0,0,0,0,1,-1,0,1,1,1,-1,0,1,-1,-1,-1,0,-1,0,0,0,-1,1,-1,0,0,1,0,0,1,-1,-1,-1,-1,1,0,1,1,1,0,-1,0,0,1,-1,1,-1,0,-1,0,0,1,-1,-1,0,0,0,1,1,0,1,1,0,0,1,-1,-1,1,1,0,0,1,0,0,0,-1,0,0,0,0,1,1,1,0,1,0,-1,0,-1,0,0,0,-1,1,-1,-1,1,1,0,1,1,1,0,0,-1,-1,0,1,0,-1,-1,-1,1,0,1,1,0,-1,0,-1,-1,1,0,0,-1,0,-1,1,1,0,-1,-1,0,1,1,-1,-1,-1,-1,0,1,0,-1,0,1,-1,1,0,1,1,1,-1,-1,0,-1,1,-1,0,1,0,-1,0,0,1,1,0,1,1,0,0,0,0,1,-1,1,-1,-1,0,-1,1,1,0,0,1,1,0,1,1,1,0,-1,0,0,1,-1,-1,1,1,0,1,-1,1,0,0,0,-1,0,0,0,1,1,0,0,0,1,-1,1,-1,0,0,0,0,-1,-1,0,0,-1,1,-1,1,-1,0,1,0,1,1,-1,0,1,-1,-1,-1,1,-1,1,1,-1,-1,0,0,1,1,1,1,-1,0],\"coloraxis\":\"coloraxis\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"showlegend\":false,\"x\":[-4.388396,-8.696331,-4.2471657,-6.219733,-7.4751487,-6.990866,-7.3711605,-4.205715,-3.4863396,-7.454163,-9.524794,-3.9230874,-4.3311987,-5.1492467,-4.276677,-6.4002075,-9.320303,-5.222484,-6.349658,-6.532165,-5.3781724,-7.071989,-7.450423,-3.898455,-6.3709116,-8.049793,-6.2880254,-4.4208827,-6.5493984,-7.473904,-5.2521095,-5.4540744,-6.4183717,-5.272907,-9.511802,-7.2671776,-4.4777784,-4.5034623,-6.6728773,-9.334737,-7.260841,-5.222045,-5.38084,-5.354629,-9.48285,-4.8929625,-5.7607155,-5.2139435,-6.9860554,-5.846614,-3.6983607,-3.9694848,-5.3950677,-9.5097885,-5.362684,-6.4878564,-8.319996,-5.233187,-7.6760497,-5.406178,-9.244259,-6.0189524,-7.0062914,-6.691225,-5.5215435,-3.9441483,-3.4974678,-6.3803678,-5.364011,-3.6929228,-4.3965254,-4.627305,-5.9285626,-7.9965463,-7.734052,-6.3206553,-4.65867,-4.4699664,-6.089728,-5.3960853,-6.632794,-7.4118342,-6.14488,-4.275628,-4.729301,-8.779483,-5.2776012,-4.402957,-5.2391715,-4.2388415,-6.3558235,-4.8835516,-4.3355517,-5.3030133,-6.437979,-5.23011,-5.2018886,-3.9093559,-4.2576356,-4.827193,-5.8275824,-4.351524,-7.476987,-7.860569,-7.797255,-5.4086432,-7.4832587,-3.5340288,-2.9471939,-9.52474,-4.617766,-4.240207,-5.291499,-5.161586,-6.141687,-5.251636,-9.427471,-4.146637,-4.792121,-6.0721803,-3.4221215,-9.009471,-6.298892,-9.515376,-6.9996443,-6.283685,-3.6482522,-6.1571937,-7.1237984,-4.263733,-4.2578588,-6.664143,-6.53594,-8.881882,-6.7760177,-5.292018,-7.1167183,-5.285587,-5.268422,-5.2768555,-5.540558,-5.0320654,-6.1267548,-7.099907,-5.368454,-2.8203654,-9.506888,-6.240439,-7.0445914,-3.6601148,-8.077126,-6.3844843,-5.3125834,-6.4340506,-4.1693816,-5.740209,-4.039909,-7.1038885,-5.230607,-6.387862,-4.313269,-6.7601085,-5.4664965,-7.168585,-9.470208,-7.038432,-6.478508,-6.093182,-6.4038343,-5.308309,-6.514968,-5.2359953,-5.283585,-7.78104,-6.028113,-5.696683,-5.1781926,-6.1393986,-7.901325,-5.4344983,-5.3436933,-6.399918,-5.311749,-3.933557,-4.793784,-3.9783936,-5.255431,-5.1297235,-5.695076,-4.8560658,-5.39943,-4.389391,-4.6052566,-5.244791,-5.268244,-6.1848173,-5.2677484,-4.9608107,-3.7966256,-3.7025318,-6.9466314,-5.3629603,-7.922211,-4.0478816,-4.8948455,-6.4174185,-5.2804794,-6.1374497,-5.254218,-5.0732613,-9.500348,-6.6934357,-8.795438,-3.688276,-6.2404213,-5.3011284,-4.7879004,-5.470968,-4.199099,-9.344182,-4.9592376,-6.347616,-9.518821,-4.4314656,-3.685227,-6.331645,-9.513971,-5.230652,-5.2704463,-3.7741845,-7.9664893,-5.1560726,-6.789221,-4.9816146,-5.305697,-4.4727764,-4.105166,-4.0598736,-5.291597,-4.8519692,-4.0002594,-3.1159914,-8.24495,-7.732164,-5.7509274,-5.8178945,-9.483497,-9.509904,-4.223292,-6.8834276,-2.8445067,-5.2652707,-4.8887243,-5.158365,-6.6778917,-6.445301,-7.170403,-6.217626,-6.031767,-3.5821698,-5.1418934,-5.1895304,-3.9356248,-4.3058786,-9.492762,-6.187832,-4.883282,-4.285535,-5.3571887,-5.5086837,-3.492566,-7.285794,-5.9072175,-5.312231,-4.8288026,-3.3631527,-3.3436172,-4.054039,-5.6841087,-4.4963937,-6.607513,-4.987873,-6.41118,-5.4412513,-3.9631007,-6.1816273,-3.656775,-6.427161,-4.885057,-8.037361,-3.6399326,-7.799535,-7.051031,-6.741552,-4.342809,-4.160206,-7.020751,-5.276333,-5.221119,-7.0280366,-8.350668,-4.01755,-5.301857,-6.330783,-6.9077396,-4.5373588,-6.8256354,-3.4884748,-6.3385587,-4.9782887,-5.4347034,-3.8576179,-3.8771136,-4.545093,-3.8330843,-6.7055063,-4.5561776,-7.6890645,-6.4116683,-5.3086743,-6.7228537,-3.4398658,-7.1698484,-5.437029,-6.1199102,-5.142345,-5.8968816,-6.2765427,-4.789327,-7.5626044,-3.908025,-3.0589983,-4.019467,-4.6347485,-6.0911884,-5.2310495,-4.067616,-6.034125,-7.760144,-7.103659,-5.5155163,-4.430103,-9.500904,-2.7424827,-4.4141955,-4.9839973,-3.7980711,-5.2598486,-6.7454557,-5.238744,-5.517597,-7.4832587,-4.389391,-5.322602,-4.844406,-4.969413,-3.4467437,-4.1867547,-7.4762793,-4.609738,-5.225829,-5.319285,-5.7754993,-6.915351,-5.225793,-5.4423037,-4.4301577,-4.925291,-3.803168,-5.319904,-6.3331065,-5.279337,-9.514241,-3.2968748,-4.7052245,-3.7480066,-7.1912827,-6.544049,-9.511191,-7.7325497,-6.032717,-3.3789592,-6.3913603,-5.2692494,-2.8183482,-4.9758663,-5.8359704,-5.343403,-5.3856487,-5.8621073,-5.1533055,-5.9323997,-6.581783,-6.549404,-5.396153,-5.61855,-6.055335,-7.2414465,-4.2320104,-5.387224,-6.385996,-7.9188924,-5.3948126,-5.451788,-5.143006,-6.4335527,-7.6561832,-5.562501,-6.4267797,-4.0905137,-8.02157,-5.285964,-4.006955,-6.906414,-3.510462,-5.986597,-3.9443069,-4.555812,-5.2837863,-5.309775,-2.7853065,-3.9538922,-4.4387646,-5.3400593,-4.312358,-5.999586,-4.115194,-6.7938795,-6.034958,-4.7761393,-4.0054255,-4.302652,-9.494188,-4.535434,-2.723403,-3.7265875,-5.3281956,-6.4605646,-5.281395,-5.9304547,-5.115128,-3.5487065,-5.291972,-5.3394847,-5.2982078,-9.482998,-5.411115,-5.7346945,-5.5777225,-6.2672234,-6.6130953,-3.4073021,-5.3334026,-5.4176397,-5.396384,-9.426929,-8.88612,-9.328562,-6.3592167,-5.230878,-4.8811297,-5.689953,-4.924863,-4.5193396,-6.329801,-6.204023,-4.8252635,-4.951092,-3.4480436,-5.219382,-4.1899967,-3.9345348,-2.9352202,-7.015623,-4.1755824,-4.912656,-4.6504426,-5.562532,-6.577841,-9.513171,-7.2740254,-6.4609222,-5.360369,-5.784131,-4.1669297,-5.7792025,-5.3572183,-7.4479833,-4.016477,-5.41022,-6.1732926,-6.354571,-4.951982,-5.1878953,-4.182358,-6.241228,-5.180957,-4.451268,-5.263453,-5.911048,-5.3011155,-5.448441,-3.1743836,-5.2494674,-9.506116,-5.3147225,-5.2757406,-5.377845,-6.413047,-3.4374478,-5.0308967,-5.7240644,-6.038942,-9.514366,-5.24616,-4.441894,-3.2011003,-6.205606,-5.2967496,-5.875346,-7.0613346,-4.388187,-6.5446367,-4.240853,-5.2507744,-6.3373013,-4.9140625,-7.5530357,-6.156888,-6.0285897,-9.511551,-5.2661157,-3.6781723,-6.1064982,-7.0067797,-5.198797,-5.206692,-5.278032,-4.5348167,-7.37508,-4.2633986,-6.4663324,-6.2104726,-5.999568,-3.9069564,-5.2804794,-8.104454,-7.066103,-4.156568,-3.6845038,-9.509503,-5.0938063,-5.327225,-5.2418227,-6.614077,-5.4079037,-4.2079067,-6.8930297,-5.2870727,-5.293902,-4.263955,-5.5555606,-3.4272122,-4.9418616,-7.380186,-6.194616,-5.255,-4.1435037,-3.8606677,-6.132272,-4.892327,-5.545024,-4.5698905,-5.2294736,-9.517977,-4.7441597,-5.1106668,-5.287357,-6.882644,-9.26164,-6.6413527,-6.9203243,-6.135355,-5.220546,-7.2124,-3.8742764,-4.4766912,-5.394838,-4.720214,-5.2596555,-7.0314074,-5.2774377,-6.2170763,-4.1109433,-4.4009686,-5.4860215,-5.2358036,-7.139484,-4.572707,-5.2938414,-5.540558,-4.8692417,-6.692227,-5.3606935,-4.746864,-4.4924645,-5.364391,-4.723739,-6.7101502,-5.364647,-7.5050263,-5.1794662,-5.373921,-5.3504314,-5.037552,-8.155313,-7.6919456,-5.3164363,-8.203024,-5.1518774,-5.540558,-6.3490887,-5.8658166,-7.582292,-5.1155243,-6.8892646,-5.4661026,-3.1776004,-9.463408,-3.609239,-3.7189796,-5.84855,-5.030602,-5.3394847,-4.3802285,-5.997761,-6.982969,-5.1845946,-6.3452835,-4.263697,-8.568323,-4.720981,-5.302004,-8.042991,-3.9630315,-6.7464466,-6.8800707,-5.3400497,-6.497933,-5.6611786,-5.739682,-7.802871,-7.8016453,-7.189462,-4.387433,-5.3075147,-7.6319346,-4.378175,-7.5233207,-6.472367,-7.249915,-4.062522,-5.365837,-9.506888,-6.195571,-5.28845,-5.6709366,-5.367635,-5.2715697,-6.8200684,-5.331722,-3.628849,-4.3855295,-6.486386,-5.331223,-9.498428,-5.535885,-4.3424277,-5.037552,-6.200095,-4.1267295,-6.001483,-4.424325,-6.258304,-5.994659,-7.991115,-6.718209,-4.3782125,-5.898692,-5.2633886,-6.436752,-3.210412,-5.093714,-5.2251577,-5.8270655,-9.493234,-4.1619277,-4.8950315,-5.211785,-4.363761,-4.7003183,-6.5392556,-5.1461067,-8.03486,-5.385039,-5.344828,-5.2432384,-5.298122,-2.7892354,-5.6302247,-5.4607716,-5.191053,-5.220635,-6.790848,-6.531871,-5.2854776,-6.2233553,-5.3176937,-5.972406,-5.498216,-6.464541,-4.3018184,-4.0039577,-5.161056,-4.9368315,-4.231153,-4.083133,-5.3432837,-9.395087,-4.0714917,-9.515396,-3.285037,-5.109638,-6.2892375,-5.2029843,-4.701149,-8.662107,-3.573386,-4.5274515,-6.1471934,-6.786876,-7.4090786,-3.2051795,-4.321339,-4.930415,-4.056851,-6.441061,-4.7184434,-7.861218,-5.2507725,-5.199711,-2.977798,-7.2550945,-4.0736475,-6.315825,-5.556513,-5.3308606,-4.690233,-5.4382157,-5.921275,-4.8223166,-5.3434377,-7.8896475,-5.1455665,-8.469561,-5.443435,-3.769166,-5.1713753,-9.506034,-4.497043,-5.8582335,-7.22453,-4.0802445,-3.9822505,-4.4914665,-7.773459,-7.51164,-4.120091,-5.121691,-9.485741,-5.2520494,-6.5084963,-8.077351,-6.16949,-5.3461466,-4.528515,-6.7938795,-5.29434,-3.878793,-5.255431,-6.57052,-4.427509,-5.1919684,-5.9726677,-5.133158,-3.6322856,-8.453747,-3.7565722,-5.998766,-6.792933,-5.9162917,-6.0062213,-7.3612843,-5.8145375,-4.1649675,-5.0120425,-8.23177,-3.9549966,-7.583184,-5.5760226,-3.2012975,-6.0617375,-6.3382277,-6.4471574,-4.5307736,-4.436101,-7.1194253,-6.177552,-5.3152843,-9.503727,-4.5977263,-6.442146,-4.7240043,-6.3066483,-6.7562327,-4.98315,-3.4326696,-5.3074045,-5.2452903,-3.7892408,-7.5403023,-2.855485,-6.22264,-5.0027566,-5.129465,-6.39574,-5.2526417,-4.794607,-5.365994,-4.904081,-5.9914017,-3.4384701,-3.808248,-4.5862956,-5.183789,-5.285587,-6.3510146,-4.602294,-3.8951404,-6.8791027,-5.3652244,-3.8130689,-6.0404825,-6.4349113,-6.05014,-5.1361003,-5.2498617,-6.235076,-4.816944,-6.9165664,-5.2118835,-6.619954,-5.1106668,-3.206974,-3.8980505,-5.2505126,-8.34937,-6.002709,-5.3806257,-5.280231,-5.8018503,-5.3929625,-7.5272894,-5.2980695,-4.8150606,-5.2700543,-3.558028,-4.910839,-7.3520646,-7.079868,-5.018491,-3.5862975,-4.2806153,-3.862546,-7.5571437,-4.253066,-5.230957,-3.5996342,-9.501009,-4.4528165,-3.768388,-3.631743,-5.246616,-4.153521,-6.4809947,-3.9348934,-5.3193803,-4.1094947,-7.386595,-4.7271013,-6.5518084,-4.2076583,-6.9964128,-2.9669225,-4.9024053,-4.886545,-4.9671392,-6.271214,-4.6691866,-5.8931184,-6.021113,-5.2579556,-5.4772787,-4.236491,-4.6693754,-7.1304574,-5.2933707,-6.972588,-3.4092295,-7.8896475,-9.520782,-5.2289643,-5.4168887,-5.944913,-5.2346215,-6.3904066,-3.809091,-6.86566,-6.6062713,-3.0249171,-5.300336,-6.247978,-4.872766,-4.660402,-7.857702,-3.858578,-4.5692396,-6.3818197,-3.0575073,-4.4108834,-6.4335527,-5.192829,-5.554968,-7.136419,-5.1450434,-5.2489114,-3.4039104,-5.1053953,-9.509981,-4.8870792,-8.700643,-7.30146,-6.408708,-5.2105246,-8.124535,-5.2652707,-5.32774,-3.8661878,-5.8021317,-5.3164363,-5.8658166,-7.661349,-4.8307724,-5.333925,-6.1016636,-4.4153113,-5.394403,-6.940641,-5.2466793,-3.932575,-7.103664,-9.489379,-4.8644156,-5.331566,-4.416604,-5.3596163,-5.8466105,-6.1471887,-4.5728803,-6.641892,-3.062747,-2.7146783,-9.481259,-6.5469027,-4.2513623,-7.7583222,-6.500956,-6.80029,-8.137054,-7.1990905,-4.620378,-7.7558727,-4.427579,-3.6351175,-6.458897,-5.3285484,-4.8672237,-6.819689,-4.6033225,-6.0414834,-6.40687,-6.9698215,-7.258175,-5.2134666,-7.7684417,-4.4100137,-5.248076,-9.490741,-3.5762544,-5.169229,-7.2577634,-4.005747,-5.26229,-6.8272495,-3.0755227,-7.0560355,-5.352936,-7.0879774,-9.485741,-3.2892914,-5.46789,-5.3163238,-5.3992496,-6.129154,-5.942153,-5.1531043,-4.636909,-5.2809377,-6.218737,-3.8890526,-4.0240927,-3.0452387,-5.3402743,-9.503601,-5.1170325,-5.3011284,-6.1570125,-6.900613,-4.400204,-6.839569,-2.553823,-6.461472,-3.949034,-5.178121,-3.4665143,-6.2084126,-5.483068,-5.374908,-5.225366,-6.222169,-3.6495366,-7.518711,-5.94114,-6.5084944,-5.3875484,-4.3819523,-6.3338714,-5.069445,-2.8017097,-5.5678096,-8.809889,-7.5693183,-5.3469915,-5.3649035,-5.48141,-9.487896,-2.6772616,-3.0864587,-6.956931,-6.2454824,-7.2328863,-7.3905473,-4.0094175,-5.247676,-5.2412205,-6.190576,-5.0287266,-4.4638386,-4.3038144,-3.6878812,-6.460891,-3.3843234,-3.0628662,-3.6502595,-5.79858,-6.246003,-6.41118,-7.940656,-6.361952,-7.32907,-4.4422493,-5.3801475,-3.6808143,-3.395551,-7.2145658,-6.090375,-4.9810004,-5.1884236,-6.498392,-6.818989,-5.322359,-5.0456758,-4.188078,-4.983443,-5.210666,-6.6674004,-6.775394,-9.509787,-5.2590184,-3.3501356,-5.2309394,-3.5352206,-7.5051622,-5.923994,-4.4931054,-7.633265,-5.321425,-5.2080746,-5.291885,-8.211146,-4.490352,-4.7223663,-3.642827,-8.078291,-4.647344,-8.189302,-7.523635,-5.202862,-6.621777,-6.110914,-5.2613735,-6.774439,-2.7192357,-7.0474277,-4.412861,-5.2884,-8.005241,-7.0298257,-4.94856,-9.476088,-6.9211426,-5.2625813,-5.865152,-6.476721,-3.4626875,-3.7560554,-8.1682625,-5.021545,-5.52632,-9.515067,-6.2466564,-5.256159,-6.822694,-9.341869,-5.2547174,-9.376449,-5.3604913,-3.9407966,-3.52148,-5.082688,-8.662107,-5.2739334,-4.327105,-4.140223,-3.7422783,-4.087954,-4.472453],\"xaxis\":\"x\",\"y\":[-3.623465,-5.172817,-6.2911744,-3.8931212,-5.322243,-4.700697,-4.759203,-3.8308048,-5.2931566,-3.5679731,-5.091115,-5.5841618,-5.5784903,-3.6332006,-5.0915055,-4.996711,-5.117455,-3.4398947,-5.836349,-5.363677,-3.5121377,-5.3012652,-3.1789255,-2.7093554,-2.5050883,-5.003706,-7.1084647,-4.2442517,-2.6008482,-2.7850904,-3.5601392,-5.7090616,-5.2832017,-3.8790677,-5.0927033,-4.436265,-3.9736845,-4.772729,-5.810403,-5.117757,-4.1915293,-3.7434008,-3.3176382,-3.4144151,-5.098331,-5.7143126,-4.0410957,-3.3825152,-3.6709998,-7.2218385,-6.0252337,-6.2459383,-6.5020742,-5.0930996,-3.440497,-3.5074804,-5.0492854,-5.2372656,-4.64598,-3.6061103,-5.12469,-7.128065,-4.9652996,-5.7086215,-7.003798,-2.7979822,-6.124033,-5.010005,-5.26514,-4.961045,-5.9658866,-5.2306457,-4.600142,-5.0184226,-5.2915726,-4.6338005,-4.690901,-6.2283072,-5.402471,-3.2925146,-5.2446218,-4.676645,-5.403333,-3.6220298,-4.4555874,-5.162587,-3.6447897,-3.926417,-3.4391212,-4.6871924,-7.1845746,-4.3280735,-5.241969,-3.508646,-5.655007,-4.814045,-3.9090261,-4.864029,-3.727401,-4.5417385,-4.693213,-5.4516416,-4.487567,-5.109167,-3.78037,-3.315444,-4.7712884,-3.4982033,-5.0473666,-5.091771,-4.601318,-4.257015,-3.385819,-3.61103,-5.357021,-4.8483605,-5.1058083,-4.8738194,-4.4972544,-3.530876,-5.0889087,-5.152,-4.3816514,-5.093181,-3.2041502,-5.9919734,-4.005285,-5.4075446,-3.9559016,-4.1939287,-3.9633017,-6.0446777,-2.4917088,-5.162822,-2.392967,-3.702163,-2.842882,-3.2781677,-3.4767516,-5.3489,-4.4787726,-4.702672,-2.485772,-5.4022737,-3.3633027,-4.116754,-5.093472,-4.381443,-6.487085,-3.4137032,-5.0610495,-5.119145,-4.9132433,-4.665677,-5.0417585,-4.754157,-6.5483603,-3.151209,-3.4455497,-3.6570797,-2.6966808,-5.650082,-6.8545914,-3.8345199,-5.0986238,-4.39046,-5.1010346,-4.898102,-2.6063085,-3.7222571,-6.140978,-3.4549406,-3.321348,-5.200131,-3.2833388,-4.5030046,-3.3367064,-4.2978067,-4.951137,-5.7780633,-3.125912,-5.6806073,-3.8279998,-3.3733993,-4.60582,-6.071829,-3.3182466,-3.5189106,-6.0790725,-6.4362254,-6.45218,-3.858122,-4.5760593,-3.3832562,-3.727773,-7.1145816,-3.4036925,-4.802513,-3.2765124,-3.6949418,-5.2045717,-3.7254133,-4.080171,-4.611843,-4.5590143,-7.260382,-3.4017675,-3.5623515,-3.7462864,-5.2987456,-5.094614,-5.7341285,-5.170249,-3.3593228,-5.07816,-3.2422838,-4.302562,-5.0085187,-4.3245153,-5.115494,-4.560202,-4.27269,-5.092346,-3.7950306,-5.208907,-5.5534196,-5.093288,-6.1899066,-3.802922,-6.0864935,-4.9912,-3.67436,-5.3427067,-6.1550965,-3.3233206,-4.0447626,-3.6143038,-4.2794685,-3.5266356,-5.1828322,-3.907707,-5.257339,-5.13602,-5.1848955,-6.9802465,-4.5497165,-5.0978346,-5.0939603,-5.6303983,-7.089919,-4.088572,-3.567077,-5.9133687,-5.92405,-2.4186876,-3.5669544,-4.4191456,-7.274949,-5.5977607,-3.4741426,-3.7497406,-6.084988,-3.5086186,-2.7981503,-5.0964046,-5.1131196,-3.1616266,-5.7905903,-3.4293773,-3.3404138,-3.4025152,-4.760135,-4.4633956,-3.248354,-4.754022,-3.4872446,-4.3545074,-5.7146745,-4.954293,-5.488561,-4.1550317,-5.3217974,-4.4685936,-3.4511485,-4.3445635,-5.821811,-4.9574656,-7.255189,-5.942739,-4.482978,-5.468332,-4.856437,-2.7480583,-4.356747,-5.2653418,-5.09571,-2.5373542,-3.3513908,-3.6720183,-5.076529,-5.2221966,-4.645178,-3.644881,-7.1894855,-6.3991137,-4.510676,-5.6553116,-5.1829643,-2.739219,-5.647831,-3.4647844,-5.957621,-4.439819,-4.736037,-5.1020956,-4.6513495,-4.5844145,-4.66722,-4.394975,-3.5346887,-5.2373443,-4.3062057,-5.291491,-3.444499,-4.74118,-3.334182,-4.010376,-7.133727,-4.542092,-5.4465075,-4.8168845,-4.480437,-3.3932984,-4.0590806,-6.9340863,-3.399117,-1.9800707,-7.3092155,-4.1107416,-2.8093665,-6.7784834,-6.1335464,-5.095614,-5.025446,-4.67134,-4.1798754,-4.906983,-3.6564775,-5.60267,-3.3597755,-5.5704503,-4.7712884,-3.858122,-3.4074657,-5.8216925,-3.008665,-4.898363,-3.9901114,-5.2167354,-4.6896973,-3.2535355,-3.378433,-4.704062,-2.6492045,-3.2384882,-6.785309,-5.2492867,-6.0025773,-2.5523944,-3.8020897,-4.2324758,-3.949087,-5.0934305,-5.77469,-4.8310757,-6.4891925,-4.3251877,-4.0003905,-5.0936046,-4.1770477,-5.1516013,-5.1375647,-2.8325002,-3.5177846,-4.610886,-5.3871317,-7.032718,-6.478697,-5.3237767,-4.5500264,-3.7757103,-4.4122596,-4.616553,-4.514889,-3.8244455,-6.5774817,-5.220068,-4.1184816,-5.4723964,-4.045788,-7.2308087,-5.3706155,-3.374025,-4.6569076,-3.1725295,-3.5057957,-5.3043756,-3.5178094,-3.2110474,-3.4722764,-5.3143945,-3.3991027,-4.6567826,-3.3132489,-3.4541461,-7.3223705,-6.357128,-4.729878,-3.0841115,-3.3003805,-3.6767263,-3.3418884,-6.982549,-3.3641758,-4.6130095,-7.2231326,-3.9681396,-4.2437234,-7.2862663,-5.123113,-2.7946405,-4.154821,-5.0961924,-5.3528543,-5.2075396,-3.5565124,-3.236848,-5.861483,-3.6688673,-6.6562605,-5.8033094,-5.173052,-3.7440116,-3.5344133,-3.3032165,-5.098242,-4.8138914,-5.1243105,-6.527515,-4.4739633,-4.377569,-6.013147,-4.730161,-3.3522809,-3.430248,-5.106178,-5.1604137,-5.120291,-3.4014323,-3.2430634,-5.3335085,-2.1278472,-6.739819,-2.8898962,-4.261643,-5.423437,-4.8005877,-5.6936364,-5.2498593,-3.1025803,-4.857837,-3.5153866,-3.4586246,-5.257664,-3.2751749,-4.3879833,-5.60369,-5.405891,-5.47471,-5.0927567,-3.906724,-3.575713,-3.9892821,-4.5625153,-4.16832,-2.2972372,-3.709679,-5.808966,-3.333346,-3.35707,-4.036355,-5.4824753,-6.2608566,-3.718459,-4.4250603,-5.253145,-3.2546046,-4.3547964,-3.6753788,-5.98797,-3.3501194,-3.2932818,-3.9957757,-3.5356054,-5.0942616,-3.901296,-3.5500202,-3.5418692,-4.788003,-5.6145625,-5.284645,-4.255917,-5.404858,-5.093,-3.2977378,-5.053484,-4.2618165,-5.5242047,-3.5426652,-5.160941,-5.573685,-2.490627,-5.689551,-4.7224855,-3.4675064,-4.8376026,-3.3949823,-5.441485,-4.1481004,-4.874569,-5.0933814,-3.5161147,-2.7147522,-4.105149,-5.407216,-3.766316,-3.4530702,-3.7071016,-5.2051806,-6.270113,-5.6819434,-4.555047,-5.3136067,-4.512407,-3.910322,-3.4017675,-3.2045588,-5.401464,-3.3378313,-2.386852,-5.0938587,-3.6713228,-3.5714128,-5.6025424,-5.984148,-6.323981,-3.8616047,-3.2631137,-3.3250422,-3.536045,-6.401845,-3.212329,-4.7466307,-4.1052,-4.1056614,-4.264135,-3.3704455,-5.316899,-4.6515894,-3.1241567,-2.606468,-5.8082094,-4.8611965,-5.678937,-5.0918803,-4.8163276,-3.3282173,-2.805693,-5.5228724,-5.1273923,-4.4297695,-4.5595427,-3.3293471,-4.6572433,-5.1230187,-2.6609676,-4.7004194,-3.502074,-4.414877,-3.4176373,-3.5355728,-3.719461,-7.28219,-3.2853315,-4.733293,-5.804604,-4.6468744,-4.749591,-4.6480317,-2.8927205,-4.4787726,-4.766502,-4.1035385,-3.7726471,-4.3896656,-5.1569977,-3.471003,-6.654306,-5.517849,-3.5181932,-5.41599,-5.655543,-4.0596933,-3.8307369,-3.3656418,-5.1543303,-5.684074,-3.266545,-5.020057,-3.654074,-4.4787726,-7.2909746,-4.2081833,-4.9798803,-5.668071,-4.393698,-6.604979,-5.121134,-5.101659,-4.720428,-4.844606,-6.7845144,-3.428043,-3.5344133,-5.484935,-5.2059317,-5.8132668,-3.4648988,-3.0254118,-2.999048,-5.1352434,-5.02402,-3.7247863,-3.4527495,-3.6752977,-4.5657396,-3.6280246,-3.6912618,-5.052012,-4.665782,-4.531193,-5.749474,-3.9750667,-2.9930415,-6.323098,-3.6079843,-4.7401333,-4.9720755,-5.659662,-5.131081,-5.2565312,-4.0389833,-3.7925582,-5.093472,-5.1650524,-3.2779045,-4.58963,-6.212209,-3.244873,-4.738133,-3.458306,-5.2564487,-5.676776,-5.4119096,-3.3819635,-5.095755,-6.907746,-4.8427205,-3.3656418,-7.233512,-4.1695604,-7.354686,-4.780102,-4.758122,-5.256559,-6.1033473,-3.0565515,-5.2839384,-4.892814,-3.2013445,-4.235547,-3.6834638,-4.8145747,-3.5516477,-7.2277484,-5.0972342,-4.08749,-4.3959365,-3.8950615,-5.6834354,-6.771041,-4.6941137,-3.2625196,-5.114923,-3.410528,-6.1741824,-3.4751694,-4.5084066,-5.3759847,-5.855507,-4.2177606,-3.4041004,-3.2830226,-4.5909753,-5.479361,-3.474117,-7.289319,-3.91283,-6.976421,-3.1049998,-5.111272,-4.3070183,-4.2481894,-3.4206984,-3.3438375,-6.147203,-6.6683125,-3.4912007,-5.110898,-3.9061003,-5.093085,-5.4837227,-3.3698564,-5.7779694,-3.4065502,-4.9579005,-5.116833,-3.7276525,-6.218178,-2.8463755,-3.499986,-3.5393252,-4.3466635,-4.9142427,-5.4351153,-5.956529,-5.6937704,-5.048356,-4.8493333,-4.0057583,-3.382224,-4.6449056,-2.9271748,-3.0030966,-5.521012,-4.2230396,-4.0469427,-5.610478,-3.346902,-4.8932676,-5.414053,-3.3425837,-4.9363284,-5.4534197,-5.1696978,-3.1888251,-4.8072248,-3.4913595,-5.0942063,-4.601815,-4.2246914,-3.231096,-2.8228822,-6.042235,-3.9185007,-5.0199585,-4.011838,-4.022225,-5.3194914,-5.0966425,-3.689219,-4.254677,-4.9844074,-5.190539,-3.5676694,-4.5136366,-4.2437234,-4.6561756,-5.393334,-3.3182466,-4.3286376,-4.690023,-3.4318023,-4.0278516,-5.5556555,-5.466477,-5.1998854,-2.4634092,-5.4244075,-2.7217076,-6.8395576,-5.746842,-5.0546937,-4.3564196,-6.7189903,-3.7405584,-5.4814553,-5.3942657,-3.8205702,-3.4837723,-3.1495,-7.3359838,-3.6780937,-4.2438426,-7.1455054,-4.4705467,-3.869256,-3.6394517,-3.2826204,-5.095162,-4.378695,-3.7536428,-4.9788237,-5.7855287,-4.465157,-4.903435,-4.9221964,-3.814879,-3.1687443,-4.8331385,-5.624074,-3.661856,-5.4593654,-5.37944,-4.8815455,-3.9044418,-3.5249565,-4.819387,-3.4385939,-3.0605397,-4.94621,-4.982413,-4.543269,-5.972666,-3.42645,-3.2781677,-5.297626,-6.0481353,-2.5846422,-5.4793634,-3.3360612,-5.1962967,-2.8160815,-5.3847013,-5.529431,-6.0031567,-3.515034,-4.978876,-5.5793138,-3.47262,-3.5081687,-3.3185112,-3.3282173,-4.410918,-3.9757888,-3.2025476,-5.0818777,-5.0504785,-6.316656,-3.23771,-4.6368756,-3.2473812,-4.7199836,-6.620583,-5.960074,-3.4343014,-3.638445,-5.437215,-3.6677108,-5.3984823,-7.2243905,-2.527664,-3.5246193,-4.377963,-3.4569697,-5.0073447,-3.663877,-4.7738748,-5.095026,-4.0674553,-5.020064,-5.247908,-3.9695892,-2.611154,-4.7512684,-5.269362,-6.1677194,-4.8544135,-6.778674,-2.4249854,-4.4169354,-4.032548,-5.09724,-5.3873677,-4.6603427,-3.400482,-3.4332862,-5.2934766,-5.0945163,-5.8072934,-4.351533,-3.4459097,-3.4558415,-4.1106043,-2.5939198,-3.8291273,-3.9794216,-5.015693,-5.9687066,-4.9363284,-5.091733,-3.3817868,-3.3534179,-4.3592134,-3.0952406,-5.817662,-3.752126,-7.107692,-4.281639,-3.87252,-3.450476,-3.1978416,-6.9318104,-4.569825,-5.232948,-4.861028,-4.4123526,-5.2877812,-4.44918,-5.125054,-3.5057957,-3.4939706,-6.384308,-4.580323,-3.606227,-3.92984,-2.5225286,-3.2800403,-5.0936456,-4.6391582,-5.1530905,-5.1544776,-5.256022,-5.7054667,-3.4972548,-3.567077,-4.052385,-4.4054656,-1.7443439,-3.266545,-4.2081833,-4.082709,-5.028702,-3.386606,-4.8064156,-4.1905513,-6.481802,-4.524746,-3.2436218,-4.959049,-3.2959025,-5.096095,-4.793139,-6.661005,-3.992538,-3.521846,-4.473903,-4.0674696,-4.9399977,-5.9119887,-4.803081,-5.0768967,-5.0975738,-5.5382133,-5.368035,-5.126211,-3.8008447,-6.0418024,-4.750026,-4.519927,-4.5883856,-4.456633,-4.301844,-4.905687,-3.0496247,-3.2335114,-4.7404323,-4.6775103,-4.619612,-5.5689754,-3.9190354,-5.3154387,-5.4370155,-4.389874,-5.046365,-5.330021,-3.266764,-5.0974245,-4.2127404,-5.1337895,-4.548543,-3.3979926,-3.9489448,-2.8029284,-3.979967,-6.063402,-3.8927312,-2.8670285,-5.0966425,-4.5510097,-6.261248,-3.878431,-3.8412693,-5.0230865,-5.2680387,-3.3639824,-5.52036,-5.3378925,-7.2829328,-3.987238,-5.959047,-5.005394,-3.4374576,-5.095117,-5.036799,-3.2422838,-5.483649,-4.4046936,-4.3610377,-5.521003,-5.291957,-4.335242,-5.3347864,-5.440549,-5.052635,-3.045123,-6.43931,-3.9116528,-3.416687,-5.742856,-3.3660722,-2.9524598,-7.3189306,-4.3225555,-3.2600214,-5.583056,-3.6007633,-7.027809,-4.2928567,-3.166367,-5.358771,-5.0366073,-3.383172,-3.2694485,-4.2110615,-5.0971923,-5.148725,-5.396268,-3.726577,-5.357793,-5.640778,-3.6188781,-2.7139866,-3.3997822,-3.408793,-5.4093,-4.866722,-4.5513783,-4.7413626,-2.6055734,-3.401103,-3.5250957,-5.375823,-5.3133054,-3.9691405,-6.3540883,-4.4685936,-4.2459025,-3.7911608,-3.296854,-3.6485348,-4.611921,-4.6914926,-4.8585434,-4.552321,-5.069756,-3.418275,-3.769344,-5.2201104,-6.816233,-4.7834773,-4.8232827,-6.333374,-5.6184964,-3.318382,-4.5167136,-3.6208744,-5.0938406,-3.2608802,-3.813551,-3.5662563,-3.3237095,-5.9324365,-4.0196114,-2.0315642,-3.7529318,-3.8386245,-3.1883457,-3.3093996,-5.174826,-4.0588446,-4.2283444,-4.9589047,-4.592445,-4.7953296,-3.714181,-4.802605,-3.6513739,-3.1727467,-4.808883,-3.7062228,-3.899576,-5.130654,-4.4396305,-6.1592336,-3.3943944,-3.1639018,-3.5029635,-6.6500216,-5.099385,-4.263265,-3.782989,-3.4583006,-5.5466075,-4.084458,-4.527893,-4.970565,-6.854825,-4.357695,-5.0925403,-5.3933372,-3.5319657,-5.127278,-5.117997,-6.2937794,-5.1129766,-3.2494726,-4.1020117,-2.8084512,-3.4432344,-5.116833,-6.041177,-3.3130102,-5.894975,-5.753795,-4.9824233,-2.240025],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"label\"}},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"t-SNE Visualization of BERT Embeddings (perplexity=583.0)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('832b090e-3ccb-4a9f-a824-9b483aafc8c3');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization of embeddings for train set\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "\n",
    "sent_embeddings_train_2D = np.array([emb[0] for emb in sent_embeddings_train])\n",
    "tsne = TSNE(n_components=2,random_state=42,perplexity=(example_number-1)/2)\n",
    "reduced_embeddings = tsne.fit_transform(sent_embeddings_train_2D)\n",
    "\n",
    "df = pd.DataFrame(reduced_embeddings, columns=['x', 'y'])\n",
    "df['label'] = labels_train  # При условии, что есть метки\n",
    "\n",
    "# Создание точечного графика\n",
    "fig = px.scatter(df, x='x', y='y', color='label', title=f't-SNE Visualization of BERT Embeddings (perplexity={(example_number-1)/2})')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gV4uiDfSt-zq",
    "outputId": "e5811daf-c9b3-4454-d52a-8d103a5a8c7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:'\n",
      " 0.5021422450728363\n",
      "Best parameters\n",
      " SVC(C=20, kernel='linear')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\n",
    "SVM=Pipeline([\n",
    "    ('normalization', Normalizer()),\n",
    "    ('SVC', SVC())])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'SVC__C': [0.1,1,5,10,20],\n",
    "    'SVC__decision_function_shape': ['ovr','ovo'],\n",
    "    'SVC__gamma': ['scale', 'auto'],\n",
    "    'SVC__kernel': ['poly', 'rbf','linear' ,'sigmoid' ] }\n",
    "\n",
    "grid = GridSearchCV(estimator=SVM, param_grid=param_grid, cv=3, scoring='accuracy',n_jobs=-1,error_score='raise')\n",
    "grid.fit(np.array(sent_embeddings_train_list), labels_train)\n",
    "\n",
    "print(\"Best estimator:'\\n\",grid.best_score_)\n",
    "print(\"Best parameters\\n\",grid.best_estimator_.named_steps['SVC'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p1bvr4v1MFJk",
    "outputId": "45efc024-3c5c-4217-f31b-7bbfba1f8546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy( with Norm):  0.51\n",
      "F1 (with Norm):  0.48\n"
     ]
    }
   ],
   "source": [
    "SVM=Pipeline([\n",
    "    ('normalization', Normalizer()),\n",
    "    ('SVC', grid.best_estimator_.named_steps['SVC'])])\n",
    "\n",
    "SVM.fit(np.array(sent_embeddings_train_list),labels_train)\n",
    "y_pred=SVM.predict(np.array(sent_embeddings_test_list))\n",
    "\n",
    "print('Accuracy( with Norm): ', round(accuracy_score(labels_test, y_pred),2))\n",
    "print('F1 (with Norm): ', round(f1_score(labels_test, y_pred, average = 'macro'),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cyk17i14sUWN",
    "outputId": "a1a5a25d-4cdd-4498-ce13-62b05f67bcb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy( with ADASYN+Norm):  0.49\n",
      "F1 (with ADASYN+Norm):  0.48\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "adsin=ADASYN(sampling_strategy='minority', n_neighbors=5, random_state=13)\n",
    "X_resampled_train,y_resampled_train = adsin.fit_resample(sent_embeddings_train_list,labels_train )\n",
    "\n",
    "SVM=Pipeline([\n",
    "    ('normalization', Normalizer()),\n",
    "    ('SVC', grid.best_estimator_.named_steps['SVC'])])\n",
    "\n",
    "SVM.fit(X_resampled_train,y_resampled_train)\n",
    "y_pred=SVM.predict(sent_embeddings_test_list)\n",
    "\n",
    "print('Accuracy( with ADASYN+Norm): ', round(accuracy_score(labels_test, y_pred),2))\n",
    "print('F1 (with ADASYN+Norm): ', round(f1_score(labels_test, y_pred, average = 'macro'),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nhu-1eVDOYl1",
    "outputId": "05d84cf8-9e59-4406-916b-81eea36023e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.48\n",
      "F1: 0.38\n"
     ]
    }
   ],
   "source": [
    "SVM=SVC()\n",
    "\n",
    "SVM.fit(np.array(sent_embeddings_train_list),labels_train)\n",
    "y_pred=SVM.predict(np.array(sent_embeddings_test_list))\n",
    "\n",
    "print('Accuracy: ', round(accuracy_score(labels_test, y_pred),2))\n",
    "print('F1:', round(f1_score(labels_test, y_pred, average = 'macro'),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6up9BpbxsUg",
    "outputId": "a881123a-b013-4737-e57b-b685e2dbccb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.43\n",
      "F1: 0.43\n"
     ]
    }
   ],
   "source": [
    "SVM= grid.best_estimator_.named_steps['SVC']\n",
    "\n",
    "SVM.fit(np.array(sent_embeddings_train_list),labels_train)\n",
    "y_pred=SVM.predict(np.array(sent_embeddings_test_list))\n",
    "\n",
    "print('Accuracy: ', round(accuracy_score(labels_test, y_pred),2))\n",
    "print('F1:', round(f1_score(labels_test, y_pred, average = 'macro'),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtRwQrg7KE1O",
    "outputId": "22fb9ea5-f196-4634-f37c-84b480c5921e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:'\n",
      " 0.50471293916024\n",
      "Best parameters\n",
      " RandomForestClassifier(max_depth=10, min_samples_split=15, n_estimators=50)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RFC=Pipeline([\n",
    "    ('normalization', Normalizer()),\n",
    "    ('RFC', RandomForestClassifier())])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'RFC__n_estimators': [30,50,100,500],\n",
    "    'RFC__max_depth': [10,20,50,100],\n",
    "    'RFC__criterion': ['entropy','gini'],\n",
    "    'RFC__min_samples_split': [2,5,10,15]}\n",
    "\n",
    "grid = GridSearchCV(estimator=RFC, param_grid=param_grid, cv=3, scoring='accuracy',n_jobs=-1,error_score='raise')\n",
    "grid.fit(np.array(sent_embeddings_train_list), labels_train)\n",
    "\n",
    "print(\"Best estimator:'\\n\",grid.best_score_)\n",
    "print(\"Best parameters\\n\",grid.best_estimator_.named_steps['RFC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "muw8fA9yJpJd",
    "outputId": "10deaea0-dea6-44bb-9d99-14593b1e716d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy( with ADASYN+Norm):  0.43\n",
      "F1 (with ADASYN+Norm):  0.43\n"
     ]
    }
   ],
   "source": [
    "adsin=ADASYN(sampling_strategy='minority', n_neighbors=5, random_state=13)\n",
    "X_resampled_train,y_resampled_train = adsin.fit_resample(sent_embeddings_train_list,labels_train )\n",
    "\n",
    "RFC=Pipeline([\n",
    "    ('normalization', Normalizer()),\n",
    "    ('RFC',grid.best_estimator_.named_steps['RFC'])])\n",
    "\n",
    "RFC.fit(X_resampled_train,y_resampled_train)\n",
    "y_pred=SVM.predict(np.array(sent_embeddings_test_list))\n",
    "\n",
    "print('Accuracy( with ADASYN+Norm): ', round(accuracy_score(labels_test, y_pred),2))\n",
    "print('F1 (with ADASYN+Norm): ', round(f1_score(labels_test, y_pred, average = 'macro'),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8Uc3G6Sv3ir",
    "outputId": "caeab994-dcad-4bdd-d62a-f42a33319d39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy( with Norm):  0.43\n",
      "F1 (with Norm):  0.43\n"
     ]
    }
   ],
   "source": [
    "RFC=Pipeline([\n",
    "    ('normalization', Normalizer()),\n",
    "    ('RFC', grid.best_estimator_.named_steps['RFC'])])\n",
    "\n",
    "RFC.fit(np.array(sent_embeddings_train_list),labels_train)\n",
    "y_pred=SVM.predict(np.array(sent_embeddings_test_list))\n",
    "\n",
    "print('Accuracy( with Norm): ', round(accuracy_score(labels_test, y_pred),2))\n",
    "print('F1 (with Norm): ', round(f1_score(labels_test, y_pred, average = 'macro'),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1Gn2ztWPTeO",
    "outputId": "8508f19b-36f2-4d0c-96b8-79f14ef895fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.43\n",
      "F1 :  0.43\n"
     ]
    }
   ],
   "source": [
    "RFC=grid.best_estimator_.named_steps['RFC']\n",
    "\n",
    "RFC.fit(np.array(sent_embeddings_train_list),labels_train)\n",
    "y_pred=SVM.predict(np.array(sent_embeddings_test_list))\n",
    "\n",
    "print('Accuracy: ', round(accuracy_score(labels_test, y_pred),2))\n",
    "print('F1 : ', round(f1_score(labels_test, y_pred, average = 'macro'),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dA9XRuUDTacH"
   },
   "source": [
    "## BERT  fine-tuning approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdGcoUYMBmzn"
   },
   "source": [
    "For the task we take BertForSequenceClassification. It's a base  BERT model with a classifier head to perform sentence or document-level classification.The BertForSequenceClassification effectively takes the BertModel and builds a classification layer on top plus some additional elements such as adding the right activation and loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnP6olGDa3jP"
   },
   "source": [
    "Support Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "t4lXVswraISU"
   },
   "outputs": [],
   "source": [
    "def flatten_accuracy(preds, labels):\n",
    "    # function that calculates accuracy for Bert model\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def flatten_f1_score(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis = 1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='macro')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9s4BumOSItQG"
   },
   "source": [
    "Data preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "gOk6k8wJ1Lyk"
   },
   "outputs": [],
   "source": [
    "# import tokenizer for BERT model\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HYHro0aGIXkR",
    "outputId": "59a80347-3e75-46cd-9317-59413319e044"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "max_length=round(mean_length_text_extract,0)\n",
    "max_length=int(max_length)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MmCiVQyh1Vf6",
    "outputId": "3c790022-c1f8-420f-ad5e-94ddabfe4a2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 2, 1])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare labels for the model\n",
    "def index_change_labels(s):\n",
    "   # a function that changes the value of labels to indexes that are understandable to BertForClassification model\n",
    "    if s == 1 :\n",
    "       s=0\n",
    "    elif s== 0:\n",
    "       s=1\n",
    "    elif s== -1:\n",
    "       s=2\n",
    "    return s\n",
    "\n",
    "train['index_BERT']= train.target.map(index_change_labels)\n",
    "labels_train= train.index_BERT.values\n",
    "labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejziERfb5tfP",
    "outputId": "30aefa36-d823-4228-fd5a-9a7a9a2f4e98"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2673: FutureWarning:\n",
      "\n",
      "The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in sentences_train:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = max_length,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )\n",
    "\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jPpg90MQM3Tw",
    "outputId": "9e77bc0c-edb5-4084-e3a9-8201503c1660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Старт дня. Забеспокоились при уходе нефти ниже $70 Индексы открылись в небольшом минусе на фоне просадки цен на нефть ниже $70 \n",
      "Token IDs: tensor([   101,    526,  13780,  10351,  22631,    119,  11712,  18106,  10513,\n",
      "         53204,  11623,  39524,  10913,    560,  23089,  10205,  10375,  58365,\n",
      "         45674,    109,  10923,  34769,  10292,  10332,  89038,  19935,    543,\n",
      "         39483,  12118,  56187,  10241,  72712,  19954,  10205,  10122, 103364,\n",
      "         12709,  99779,  15725,  10267,  10122, 102023,  45674,    109,  10923,\n",
      "           102,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0])\n"
     ]
    }
   ],
   "source": [
    "print('Original: ', sentences_train[3])\n",
    "print('Token IDs:', input_ids[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JRKcNdVr1jfp",
    "outputId": "77dca377-8116-43c6-cadf-e6fb03849fe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050 training samples\n",
      "117 validation samples\n"
     ]
    }
   ],
   "source": [
    "# devide to train and validation set in proportion 90% vs 10%\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "dev_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, dev_size])\n",
    "\n",
    "print(f'{train_size} training samples')\n",
    "print(f'{dev_size} validation samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1Ulln4H6Sce",
    "outputId": "eecceb09-d07c-4eef-8eab-109e3e8e983f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size,drop_last=True\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = batch_size,drop_last=True\n",
    "        )\n",
    "len(validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "0RJRfpI_IM8-"
   },
   "outputs": [],
   "source": [
    "# preparing data for test set\n",
    "test['index_BERT']= test.target.map(index_change_labels)\n",
    "labels_test= test.index_BERT.values\n",
    "labels_test\n",
    "\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in sentences_test:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = max_length,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )\n",
    "\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels_test)\n",
    "\n",
    "\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNY63IE-JJOh"
   },
   "source": [
    "Model inizialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XTyK8lTYJNPo",
    "outputId": "506ed063-b5dd-4122-f733-f6e81f61be26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased',num_labels = 3,output_attentions = False,output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "id": "tzjZDZRV13_c"
   },
   "outputs": [],
   "source": [
    "# inicialization of the model\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.cuda()\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "tvZLW9bp7llQ"
   },
   "outputs": [],
   "source": [
    "# Set an optimizer\n",
    "\n",
    "def set_optimizer(lr = 2e-5, dft_rate=1.2, correct_bias=True, eps = 1e-8, discriminative= True):\n",
    "      # discrimination rate is governed by dft_rate in discriminative fine-tuning\n",
    "      no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight'] #  weights will not be applied to the parameters whose names include these tokens.\n",
    "\n",
    "\n",
    "      if discriminative == True:\n",
    "\n",
    "                encoder_params = []\n",
    "                for i in range(12):\n",
    "                    encoder_decay = {\n",
    "                        'params': [p for n, p in list(model.bert.encoder.layer[i].named_parameters()) if\n",
    "                                  not any(nd in n for nd in no_decay)],\n",
    "                        'weight_decay': 0.01,\n",
    "                        'lr': lr / (dft_rate ** (12 - i))}\n",
    "                    encoder_nodecay = {\n",
    "                        'params': [p for n, p in list(model.bert.encoder.layer[i].named_parameters()) if\n",
    "                                  any(nd in n for nd in no_decay)],\n",
    "                        'weight_decay': 0.0,\n",
    "                        'lr': lr / (dft_rate ** (12 - i))}\n",
    "                    encoder_params.append(encoder_decay)\n",
    "                    encoder_params.append(encoder_nodecay)\n",
    "\n",
    "                optimizer_grouped_parameters = [\n",
    "                    {'params': [p for n, p in list(model.bert.embeddings.named_parameters()) if\n",
    "                                not any(nd in n for nd in no_decay)],\n",
    "                    'weight_decay': 0.01,\n",
    "                    'lr': lr / (dft_rate ** 13)},\n",
    "                    {'params': [p for n, p in list(model.bert.embeddings.named_parameters()) if\n",
    "                                any(nd in n for nd in no_decay)],\n",
    "                    'weight_decay': 0.0,\n",
    "                    'lr': lr / (dft_rate ** 13)},\n",
    "                    {'params': [p for n, p in list(model.bert.pooler.named_parameters()) if\n",
    "                                not any(nd in n for nd in no_decay)],\n",
    "                    'weight_decay': 0.01,\n",
    "                    'lr': lr},\n",
    "                    {'params': [p for n, p in list(model.bert.pooler.named_parameters()) if\n",
    "                                any(nd in n for nd in no_decay)],\n",
    "                    'weight_decay': 0.0,\n",
    "                    'lr': lr},\n",
    "                    {'params': [p for n, p in list(model.classifier.named_parameters()) if\n",
    "                                not any(nd in n for nd in no_decay)],\n",
    "                    'weight_decay': 0.01,\n",
    "                    'lr': lr},\n",
    "                    {'params': [p for n, p in list(model.classifier.named_parameters()) if any(nd in n for nd in no_decay)],\n",
    "                    'weight_decay': 0.0,\n",
    "                    'lr': lr}]\n",
    "\n",
    "                optimizer_grouped_parameters.extend(encoder_params)\n",
    "\n",
    "\n",
    "      else:\n",
    "                param_optimizer = list(model.named_parameters())\n",
    "\n",
    "                optimizer_grouped_parameters = [\n",
    "                    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "                    'weight_decay': 0.01},\n",
    "                    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "                ]\n",
    "\n",
    "      return AdamW(optimizer_grouped_parameters,\n",
    "                    lr=lr,\n",
    "                    eps = eps,\n",
    "                    correct_bias=correct_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "oV0iO1c-VSD7"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# set sheduler\n",
    "\n",
    "def set_scheduler (train_dataloader,epochs,optimizer, STLR = True) :\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    if STLR == True:\n",
    "       warm_up_proportion=0.1\n",
    "       num_warmup_steps = int(float(total_steps) * warm_up_proportion)\n",
    "    else:\n",
    "       num_warmup_steps=0\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps,num_training_steps = total_steps)\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "mcpbmJxMv9Uc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "\n",
    "# training the model\n",
    "import random\n",
    "\n",
    "def train(model,train_dataloader,validation_dataloader,optimizer,scheduler,device,epochs=4,seed_val=42,gradual_unfreeze=True,encoder_no=1):\n",
    "\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "\n",
    "    training_stats = []\n",
    "    i=0\n",
    "    step_number = len(train_dataloader)\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(f'Epoch {epoch_i} : ')\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # graduate unfrizing\n",
    "            if (gradual_unfreeze and i == 0):\n",
    "                for param in model.bert.parameters():\n",
    "                    param.requires_grad = False\n",
    "            if (step % (step_number // 3)) == 0:\n",
    "                i += 1\n",
    "            if (gradual_unfreeze and i > 1 and i < encoder_no):\n",
    "                for k in range(i - 1):\n",
    "                    try:\n",
    "                        for param in model.bert.encoder.layer[encoder_no - 1 - k].parameters():\n",
    "                            param.requires_grad = True\n",
    "                    except:\n",
    "                        pass\n",
    "            if (gradual_unfreeze and i > encoder_no + 1):\n",
    "                for param in model.bert.embeddings.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            #main loop\n",
    "            if step % 10 == 0 and not step == 0:\n",
    "                # Report progress.\n",
    "                print(' Epoch {:>5,}  Batch {:>5,}  of  {:>5,}.   '.format(epoch_i,step, len(train_dataloader)))\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels=batch[2].to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "\n",
    "            output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,labels=b_labels)\n",
    "            loss = output[0]\n",
    "            logits = output[1]\n",
    "\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Validation:\")\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        total_f1 =0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        for batch in validation_dataloader:\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels=batch[2].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,labels=b_labels)\n",
    "                loss = output[0]\n",
    "                logits = output[1]\n",
    "\n",
    "\n",
    "\n",
    "            total_eval_loss += loss.item()\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            total_eval_accuracy += flatten_accuracy(logits, label_ids)\n",
    "            total_f1+=flatten_f1_score(logits, label_ids)\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "\n",
    "        # Calculate the average F1 over all of the batches.\n",
    "        avg_val_f1 = total_f1 / len(validation_dataloader)\n",
    "\n",
    "        print(\"  Validation F1: {0:.2f}\".format(avg_val_f1),'',sep='\\n')\n",
    "\n",
    "        training_stats.append(\n",
    "        {\n",
    "            'Epoch': epoch_i + 1,\n",
    "            'Train. Loss': round(avg_train_loss,2),\n",
    "            'Valid. Loss': round(avg_val_loss,2),\n",
    "            'Valid. Accur.': round(avg_val_accuracy,2),\n",
    "            'Valid. F1': round(avg_val_f1,2),\n",
    "            'Number_of_epochs': epochs,\n",
    "        }\n",
    "        )\n",
    "\n",
    "    return model,training_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "s5q1urKx3gs_"
   },
   "outputs": [],
   "source": [
    "# Prediction on the Test set\n",
    "def evaluation(model,prediction_dataloader):\n",
    "\n",
    "      valid_stats=[]\n",
    "      model.eval()\n",
    "\n",
    "      predictions , true_labels = [], []\n",
    "\n",
    "      # Predict\n",
    "      for batch in prediction_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "      from sklearn.metrics import accuracy_score\n",
    "\n",
    "      flat_predictions = np.concatenate(predictions, axis=0)\n",
    "      flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "      flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "      valid_stats.append(\n",
    "        {\n",
    "            'Accuracy': round(accuracy_score(flat_true_labels, flat_predictions),2),\n",
    "            'F1 Score': round(f1_score(flat_true_labels, flat_predictions, average='macro'),2)\n",
    "\n",
    "        }\n",
    "        )\n",
    "\n",
    "      print(\"Metrics for Test set: \")\n",
    "      print(\"Accuracy: \", round(accuracy_score(flat_true_labels, flat_predictions),2))\n",
    "      print(\"F1 Score: \", round(f1_score(flat_true_labels, flat_predictions, average='macro'),2),'',sep='\\n')\n",
    "\n",
    "      return valid_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5OXq8d_W4-n_",
    "outputId": "5518ee75-da5e-4bb6-a8a8-fffde1dbf79a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs : 2\n",
      "Epoch 0 : \n",
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 1.03\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.42\n",
      "  Validation Loss: 1.04\n",
      "  Validation F1: 0.38\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.97\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.44\n",
      "  Validation Loss: 1.02\n",
      "  Validation F1: 0.38\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.46\n",
      "F1 Score: \n",
      "0.37\n",
      "\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.77\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.44\n",
      "  Validation Loss: 1.08\n",
      "  Validation F1: 0.44\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.81\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.48\n",
      "  Validation Loss: 1.02\n",
      "  Validation F1: 0.48\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.49\n",
      "F1 Score: \n",
      "0.48\n",
      "\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.51\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.50\n",
      "  Validation Loss: 1.04\n",
      "  Validation F1: 0.50\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.67\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.47\n",
      "  Validation Loss: 1.07\n",
      "  Validation F1: 0.47\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.51\n",
      "F1 Score: \n",
      "0.51\n",
      "\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.42\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.45\n",
      "  Validation Loss: 1.07\n",
      "  Validation F1: 0.45\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.61\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.47\n",
      "  Validation Loss: 1.10\n",
      "  Validation F1: 0.47\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.49\n",
      "F1 Score: \n",
      "0.48\n",
      "\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.30\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.47\n",
      "  Validation Loss: 1.43\n",
      "  Validation F1: 0.47\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.52\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.47\n",
      "  Validation Loss: 1.44\n",
      "  Validation F1: 0.47\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.49\n",
      "F1 Score: \n",
      "0.49\n",
      "\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.17\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.44\n",
      "  Validation Loss: 1.57\n",
      "  Validation F1: 0.43\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.46\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.45\n",
      "  Validation Loss: 1.59\n",
      "  Validation F1: 0.45\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.49\n",
      "F1 Score: \n",
      "0.49\n",
      "\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.17\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.34\n",
      "  Validation Loss: 1.77\n",
      "  Validation F1: 0.34\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.44\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.38\n",
      "  Validation Loss: 1.66\n",
      "  Validation F1: 0.38\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.5\n",
      "F1 Score: \n",
      "0.49\n",
      "\n",
      "Number of epochs : 3\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.12\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.45\n",
      "  Validation Loss: 1.52\n",
      "  Validation F1: 0.43\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.41\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.41\n",
      "  Validation Loss: 1.60\n",
      "  Validation F1: 0.41\n",
      "\n",
      "Epoch 2 : \n",
      " Epoch     2  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.85\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.39\n",
      "  Validation Loss: 1.53\n",
      "  Validation F1: 0.40\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.48\n",
      "F1 Score: \n",
      "0.47\n",
      "\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.07\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.45\n",
      "  Validation Loss: 1.64\n",
      "  Validation F1: 0.42\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.30\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.47\n",
      "  Validation Loss: 1.80\n",
      "  Validation F1: 0.47\n",
      "\n",
      "Epoch 2 : \n",
      " Epoch     2  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.72\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.47\n",
      "  Validation Loss: 1.69\n",
      "  Validation F1: 0.47\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.46\n",
      "F1 Score: \n",
      "0.45\n",
      "\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.04\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.45\n",
      "  Validation Loss: 1.71\n",
      "  Validation F1: 0.45\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.17\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.47\n",
      "  Validation Loss: 1.88\n",
      "  Validation F1: 0.47\n",
      "\n",
      "Epoch 2 : \n",
      " Epoch     2  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.59\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.52\n",
      "  Validation Loss: 1.80\n",
      "  Validation F1: 0.52\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.49\n",
      "F1 Score: \n",
      "0.48\n",
      "\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.03\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.50\n",
      "  Validation Loss: 1.74\n",
      "  Validation F1: 0.50\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.11\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.47\n",
      "  Validation Loss: 1.95\n",
      "  Validation F1: 0.47\n",
      "\n",
      "Epoch 2 : \n",
      " Epoch     2  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.45\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.45\n",
      "  Validation Loss: 1.97\n",
      "  Validation F1: 0.45\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.48\n",
      "F1 Score: \n",
      "0.47\n",
      "\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.02\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.47\n",
      "  Validation Loss: 2.50\n",
      "  Validation F1: 0.47\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.07\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.45\n",
      "  Validation Loss: 2.98\n",
      "  Validation F1: 0.45\n",
      "\n",
      "Epoch 2 : \n",
      " Epoch     2  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.45\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.45\n",
      "  Validation Loss: 2.62\n",
      "  Validation F1: 0.46\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.49\n",
      "F1 Score: \n",
      "0.48\n",
      "\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.01\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.48\n",
      "  Validation Loss: 2.70\n",
      "  Validation F1: 0.49\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.03\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.44\n",
      "  Validation Loss: 2.85\n",
      "  Validation F1: 0.44\n",
      "\n",
      "Epoch 2 : \n",
      " Epoch     2  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.40\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.45\n",
      "  Validation Loss: 2.89\n",
      "  Validation F1: 0.45\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.48\n",
      "F1 Score: \n",
      "0.47\n",
      "\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.01\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.47\n",
      "  Validation Loss: 2.71\n",
      "  Validation F1: 0.47\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.04\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.44\n",
      "  Validation Loss: 3.09\n",
      "  Validation F1: 0.44\n",
      "\n",
      "Epoch 2 : \n",
      " Epoch     2  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.42\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.45\n",
      "  Validation Loss: 2.79\n",
      "  Validation F1: 0.45\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.45\n",
      "F1 Score: \n",
      "0.45\n",
      "\n",
      "Number of epochs : 6\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.00\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.55\n",
      "  Validation Loss: 2.74\n",
      "  Validation F1: 0.55\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.05\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.44\n",
      "  Validation Loss: 2.98\n",
      "  Validation F1: 0.44\n",
      "\n",
      "Epoch 2 : \n",
      " Epoch     2  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.40\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.53\n",
      "  Validation Loss: 2.63\n",
      "  Validation F1: 0.53\n",
      "\n",
      "Epoch 3 : \n",
      " Epoch     3  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.94\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.55\n",
      "  Validation Loss: 2.39\n",
      "  Validation F1: 0.54\n",
      "\n",
      "Epoch 4 : \n",
      " Epoch     4  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.68\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.58\n",
      "  Validation Loss: 2.35\n",
      "  Validation F1: 0.57\n",
      "\n",
      "Epoch 5 : \n",
      " Epoch     5  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.71\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.52\n",
      "  Validation Loss: 2.35\n",
      "  Validation F1: 0.51\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.5\n",
      "F1 Score: \n",
      "0.49\n",
      "\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.00\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.56\n",
      "  Validation Loss: 2.51\n",
      "  Validation F1: 0.56\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.03\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.55\n",
      "  Validation Loss: 2.69\n",
      "  Validation F1: 0.54\n",
      "\n",
      "Epoch 2 : \n",
      " Epoch     2  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.20\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.48\n",
      "  Validation Loss: 2.72\n",
      "  Validation F1: 0.49\n",
      "\n",
      "Epoch 3 : \n",
      " Epoch     3  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.42\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.48\n",
      "  Validation Loss: 2.52\n",
      "  Validation F1: 0.49\n",
      "\n",
      "Epoch 4 : \n",
      " Epoch     4  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.39\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.50\n",
      "  Validation Loss: 2.57\n",
      "  Validation F1: 0.50\n",
      "\n",
      "Epoch 5 : \n",
      " Epoch     5  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.53\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.52\n",
      "  Validation Loss: 2.56\n",
      "  Validation F1: 0.51\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.48\n",
      "F1 Score: \n",
      "0.47\n",
      "\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.00\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.47\n",
      "  Validation Loss: 2.87\n",
      "  Validation F1: 0.46\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.02\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.45\n",
      "  Validation Loss: 3.14\n",
      "  Validation F1: 0.45\n",
      "\n",
      "Epoch 2 : \n",
      " Epoch     2  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.12\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.47\n",
      "  Validation Loss: 3.26\n",
      "  Validation F1: 0.47\n",
      "\n",
      "Epoch 3 : \n",
      " Epoch     3  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.21\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.56\n",
      "  Validation Loss: 2.65\n",
      "  Validation F1: 0.56\n",
      "\n",
      "Epoch 4 : \n",
      " Epoch     4  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.21\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.52\n",
      "  Validation Loss: 2.63\n",
      "  Validation F1: 0.52\n",
      "\n",
      "Epoch 5 : \n",
      " Epoch     5  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.38\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.53\n",
      "  Validation Loss: 2.51\n",
      "  Validation F1: 0.53\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.49\n",
      "F1 Score: \n",
      "0.49\n",
      "\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.00\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.48\n",
      "  Validation Loss: 3.01\n",
      "  Validation F1: 0.48\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.00\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.50\n",
      "  Validation Loss: 3.18\n",
      "  Validation F1: 0.50\n",
      "\n",
      "Epoch 2 : \n",
      " Epoch     2  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.07\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.52\n",
      "  Validation Loss: 2.84\n",
      "  Validation F1: 0.51\n",
      "\n",
      "Epoch 3 : \n",
      " Epoch     3  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.10\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.55\n",
      "  Validation Loss: 2.87\n",
      "  Validation F1: 0.53\n",
      "\n",
      "Epoch 4 : \n",
      " Epoch     4  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.08\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.58\n",
      "  Validation Loss: 2.63\n",
      "  Validation F1: 0.57\n",
      "\n",
      "Epoch 5 : \n",
      " Epoch     5  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.28\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.56\n",
      "  Validation Loss: 2.64\n",
      "  Validation F1: 0.56\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.48\n",
      "F1 Score: \n",
      "0.48\n",
      "\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.00\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.53\n",
      "  Validation Loss: 3.61\n",
      "  Validation F1: 0.53\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.01\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.45\n",
      "  Validation Loss: 4.10\n",
      "  Validation F1: 0.43\n",
      "\n",
      "Epoch 2 : \n",
      " Epoch     2  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.06\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.55\n",
      "  Validation Loss: 3.66\n",
      "  Validation F1: 0.55\n",
      "\n",
      "Epoch 3 : \n",
      " Epoch     3  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.06\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.53\n",
      "  Validation Loss: 3.86\n",
      "  Validation F1: 0.53\n",
      "\n",
      "Epoch 4 : \n",
      " Epoch     4  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.09\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.50\n",
      "  Validation Loss: 3.58\n",
      "  Validation F1: 0.49\n",
      "\n",
      "Epoch 5 : \n",
      " Epoch     5  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.28\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.52\n",
      "  Validation Loss: 3.58\n",
      "  Validation F1: 0.51\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.5\n",
      "F1 Score: \n",
      "0.49\n",
      "\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.00\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.55\n",
      "  Validation Loss: 3.79\n",
      "  Validation F1: 0.55\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.00\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.56\n",
      "  Validation Loss: 4.10\n",
      "  Validation F1: 0.56\n",
      "\n",
      "Epoch 2 : \n",
      " Epoch     2  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.00\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.53\n",
      "  Validation Loss: 4.09\n",
      "  Validation F1: 0.53\n",
      "\n",
      "Epoch 3 : \n",
      " Epoch     3  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.02\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.53\n",
      "  Validation Loss: 4.11\n",
      "  Validation F1: 0.52\n",
      "\n",
      "Epoch 4 : \n",
      " Epoch     4  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.01\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.52\n",
      "  Validation Loss: 4.09\n",
      "  Validation F1: 0.51\n",
      "\n",
      "Epoch 5 : \n",
      " Epoch     5  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.29\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.52\n",
      "  Validation Loss: 4.15\n",
      "  Validation F1: 0.51\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.48\n",
      "F1 Score: \n",
      "0.48\n",
      "\n",
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     0  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.00\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.55\n",
      "  Validation Loss: 4.30\n",
      "  Validation F1: 0.54\n",
      "\n",
      "Epoch 1 : \n",
      " Epoch     1  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.00\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.52\n",
      "  Validation Loss: 4.81\n",
      "  Validation F1: 0.52\n",
      "\n",
      "Epoch 2 : \n",
      " Epoch     2  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.01\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.53\n",
      "  Validation Loss: 4.73\n",
      "  Validation F1: 0.53\n",
      "\n",
      "Epoch 3 : \n",
      " Epoch     3  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.02\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.58\n",
      "  Validation Loss: 4.30\n",
      "  Validation F1: 0.56\n",
      "\n",
      "Epoch 4 : \n",
      " Epoch     4  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.01\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.56\n",
      "  Validation Loss: 4.63\n",
      "  Validation F1: 0.56\n",
      "\n",
      "Epoch 5 : \n",
      " Epoch     5  Batch    10  of     16.   \n",
      "\n",
      "  Average training loss: 0.32\n",
      "\n",
      "Validation:\n",
      "  Accuracy: 0.55\n",
      "  Validation Loss: 4.58\n",
      "  Validation F1: 0.54\n",
      "\n",
      "Metrics for Test set: \n",
      "Accuracy:  0.47\n",
      "F1 Score: \n",
      "0.46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# experiments\n",
    "\n",
    "\n",
    "for epochs in [2,3,6]:\n",
    "      print ('Number of epochs :',epochs)\n",
    "\n",
    "      # domain train model (with decay)\n",
    "      name='model with Decay'\n",
    "\n",
    "      optimizer = set_optimizer(lr = 2e-5,eps = 1e-8, discriminative= False)\n",
    "      scheduler=set_scheduler(train_dataloader,epochs,optimizer,STLR = False)\n",
    "\n",
    "      model_D, result_D = train(model,train_dataloader,validation_dataloader,optimizer,scheduler,device,epochs=epochs,gradual_unfreeze=False)\n",
    "      mertics=evaluation(model=model_D,prediction_dataloader=prediction_dataloader)\n",
    "\n",
    "      df_stats1 = pd.DataFrame (data=result_D)\n",
    "\n",
    "      df_stats1['model']=name\n",
    "\n",
    "      df_stats1_1 = pd.DataFrame (data=mertics)\n",
    "      df_stats1_1['model']=name\n",
    "\n",
    "\n",
    "\n",
    "      # domain train model (with decay, STLR )\n",
    "\n",
    "      name='model with Decay,SLTR'\n",
    "\n",
    "      optimizer = set_optimizer(lr = 2e-5,eps = 1e-8, discriminative= False)\n",
    "      scheduler=set_scheduler(train_dataloader,epochs,optimizer,STLR = True)\n",
    "\n",
    "      model_D_SLTR, result_D_SLTR = train(model,train_dataloader,validation_dataloader,optimizer,scheduler,device,epochs=epochs,gradual_unfreeze=False)\n",
    "      mertics=evaluation(model=model_D_SLTR,prediction_dataloader=prediction_dataloader)\n",
    "\n",
    "      df_stats2 = pd.DataFrame (data=result_D_SLTR)\n",
    "\n",
    "      df_stats2['model']=name\n",
    "\n",
    "      df_stats2_1 = pd.DataFrame (data=mertics)\n",
    "      df_stats2_1['model']=name\n",
    "\n",
    "\n",
    "      # domain train model (with decay, STLR,GU(12) )\n",
    "\n",
    "      name='model with Decay,SLTR,GU(12 layers)'\n",
    "\n",
    "      optimizer = set_optimizer(lr = 2e-5,eps = 1e-8, discriminative= False)\n",
    "      scheduler=set_scheduler(train_dataloader,epochs,optimizer,STLR = True)\n",
    "\n",
    "      model_D_SLTR_GU12, result_D_SLTR_GU12 = train(model,train_dataloader,validation_dataloader,optimizer,scheduler,device,epochs=epochs,gradual_unfreeze=True,encoder_no=12)\n",
    "      mertics=evaluation(model=model_D_SLTR_GU12,prediction_dataloader=prediction_dataloader)\n",
    "\n",
    "      df_stats3 = pd.DataFrame (data=result_D_SLTR_GU12)\n",
    "\n",
    "      df_stats3['model']=name\n",
    "\n",
    "      df_stats3_1 = pd.DataFrame (data=mertics)\n",
    "      df_stats3_1['model']=name\n",
    "\n",
    "\n",
    "      # domain train model (with decay, STLR,GU(6) )\n",
    "\n",
    "      name='model with Decay,SLTR,GU(6 layers)'\n",
    "\n",
    "      optimizer = set_optimizer(lr = 2e-5,eps = 1e-8, discriminative= False)\n",
    "      scheduler=set_scheduler(train_dataloader,epochs,optimizer,STLR = True)\n",
    "\n",
    "      model_D_SLTR_GU6, result_D_SLTR_GU6 = train(model,train_dataloader,validation_dataloader,optimizer,scheduler,device,epochs=epochs,gradual_unfreeze=True,encoder_no=6)\n",
    "      mertics=evaluation(model=model_D_SLTR_GU6,prediction_dataloader=prediction_dataloader)\n",
    "\n",
    "      df_stats4 = pd.DataFrame (data=result_D_SLTR_GU6)\n",
    "\n",
    "      df_stats4['model']=name\n",
    "\n",
    "      df_stats4_1 = pd.DataFrame (data=mertics)\n",
    "      df_stats4_1['model']=name\n",
    "\n",
    "\n",
    "      # domain train model (with decay, STLR,GU(12),DFT )\n",
    "\n",
    "      name='model with Decay,SLTR,GU(12 layers),DFT'\n",
    "\n",
    "      optimizer = set_optimizer(lr = 2e-5,dft_rate=1.5,eps = 1e-8, discriminative= True, correct_bias=False)\n",
    "      scheduler=set_scheduler(train_dataloader,epochs,optimizer,STLR = True)\n",
    "\n",
    "      model_D_SLTR_GU12_DFT, result_D_SLTR_GU12_DFT = train(model,train_dataloader,validation_dataloader,optimizer,scheduler,device,epochs=epochs,gradual_unfreeze=True,encoder_no=12)\n",
    "      mertics=evaluation(model=model_D_SLTR_GU12_DFT,prediction_dataloader=prediction_dataloader)\n",
    "\n",
    "      df_stats5 = pd.DataFrame (data=result_D_SLTR_GU12_DFT)\n",
    "\n",
    "      df_stats5['model']=name\n",
    "\n",
    "      df_stats5_1 = pd.DataFrame (data=mertics)\n",
    "      df_stats5_1['model']=name\n",
    "\n",
    "\n",
    "\n",
    "      # domain train model (with decay, STLR,GU(6),DFT )\n",
    "\n",
    "      name='model with Decay,SLTR,GU(6 layers),DFT'\n",
    "\n",
    "      optimizer = set_optimizer(lr = 2e-5,dft_rate=1.5,eps = 1e-8, discriminative= True, correct_bias=False)\n",
    "      scheduler=set_scheduler(train_dataloader,epochs,optimizer,STLR = True)\n",
    "\n",
    "      model_D_SLTR_GU6_DFT, result_D_SLTR_GU6_DFT = train(model,train_dataloader,validation_dataloader,optimizer,scheduler,device,epochs=epochs,gradual_unfreeze=True,encoder_no=6)\n",
    "      mertics=evaluation(model=model_D_SLTR_GU6_DFT,prediction_dataloader=prediction_dataloader)\n",
    "\n",
    "      df_stats6 = pd.DataFrame (data=result_D_SLTR_GU6_DFT)\n",
    "\n",
    "      df_stats6['model']=name\n",
    "\n",
    "      df_stats6_1 = pd.DataFrame (data=mertics)\n",
    "      df_stats6_1['model']=name\n",
    "\n",
    "\n",
    "      # vanilla model\n",
    "\n",
    "      total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "      name='vanilla model'\n",
    "\n",
    "      optimizer = AdamW(model.parameters(),lr = 2e-5,eps = 1e-8)\n",
    "      scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0,num_training_steps = total_steps)\n",
    "\n",
    "      model_vanilla, result_vanilla = train(model,train_dataloader,validation_dataloader,optimizer,scheduler,device,epochs=epochs,gradual_unfreeze=False)\n",
    "      mertics=evaluation(model=model_vanilla,prediction_dataloader=prediction_dataloader)\n",
    "\n",
    "      df_stats7 = pd.DataFrame (data=result_vanilla)\n",
    "\n",
    "      df_stats7['model']=name\n",
    "\n",
    "      df_stats7_1 = pd.DataFrame (data=mertics)\n",
    "      df_stats7_1['model']=name\n",
    "\n",
    "\n",
    "      df_train= pd.concat([df_stats1,df_stats2,df_stats3,df_stats4,df_stats5,df_stats6,df_stats7])\n",
    "      df_test= pd.concat([df_stats1_1,df_stats2_1,df_stats3_1,df_stats4_1,df_stats5_1,df_stats6_1,df_stats7_1])\n",
    "\n",
    "      if epochs==2:\n",
    "         statistica_train=df_train\n",
    "         statistica_test=df_test\n",
    "\n",
    "statistica_train=pd.concat([statistica_train,df_train])\n",
    "statistica_test=pd.concat([statistica_test,df_test])\n",
    "\n",
    "statistica_train.to_excel('data_exp1_train3_1.5_64_1459.xlsx',index=False)\n",
    "statistica_test.to_excel('data_exp1_test3_1.5_64_1459.xlsx',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
